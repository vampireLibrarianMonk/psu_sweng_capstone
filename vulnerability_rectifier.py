import argparse
import os
import sys

import autoimport
from llama_cpp import Llama

from utilities_llm import (setup_logger,
                           get_optimal_gpu_layers,
                           get_block_count_keys,
                           validate_bandit_allowance,
                           parse_python_script, run_mitigation_loop,
                           organize_imports_and_globals, write_to_file, process_streamed_output, extract_code_block,
                           extract_libraries, perform_scans, is_python_file, count_directories_in_path,
                           number_to_excel_column, create_chat_completion_llm, generate_import_statement,
                           get_methods_from_file, get_methods_with_signatures
                           )

mitigation_folder = "mitigated_files"

os.makedirs(mitigation_folder, exist_ok=True)

generated_unit_test_dir = "generated_unit_test"

os.makedirs(generated_unit_test_dir, exist_ok=True)

# Specify the path to your GGUF model file
model_path = "models/llama-3.2-3b-instruct-q8_0.gguf"

# Set up argument parser
parser = argparse.ArgumentParser(description='Process a Python file for vulnerability assessment.')
parser.add_argument('file_path', help='Path to the Python file to be analyzed.')
parser.add_argument('bandit_allowance', type=validate_bandit_allowance, help='Allowed number of Bandit '
                                                                             'mitigation runs (must be a positive '
                                                                             'integer).')

# Parse command-line arguments
args = parser.parse_args()

input_file_path = args.file_path
bandit_allowance = args.bandit_allowance

# Vet the input file as a python script file
if is_python_file(input_file_path):
    with open(input_file_path, 'r') as file:
        original_code = file.read()
else:
    print(f"{input_file_path} is not a valid Python file.")
    sys.exit(1)

# Get and separate file name into its base name and extension
base_name = os.path.basename(input_file_path)
name, ext = os.path.splitext(base_name)

# Read the specified file and use ast to parse it into a map for quicker analysis and mitigation
parsed_script = parse_python_script(input_file_path)

new_parsed_script = {
    'global_variables': '',
    'methods': {},
    'main_script': ''
}

# Get method count of parsed script
method_count = len(parsed_script['methods'].items())

# Define base name for mitigated files
mitigated_base_name = f"{name}_mitigated"

# Get the file count
file_count = count_directories_in_path(os.path.join(mitigation_folder, mitigated_base_name))

# Create folder for mitigated files
associated_letter_conversion = number_to_excel_column(file_count)
mitigated_folder = os.path.join(mitigation_folder, mitigated_base_name, associated_letter_conversion)

# Initialize the logger
logger = setup_logger(f"vulnerability_rectifier", associated_letter_conversion, name)

os.makedirs(mitigated_folder, exist_ok=True)

# Alternative way for getting block count using Llama
# Load the model
# llm = Llama(model_path=model_path)

# Access the model's metadata
# metadata = llm.metadata

# Retrieve the block count
# total_layers = int(metadata.get('llama.block_count'))

# Get the total layers from the input model
total_layers = get_block_count_keys(model_path, logger)

# Determine the optimal number of GPU layers to offload
n_gpu_layers = get_optimal_gpu_layers(model_path, total_layers)

# Initialize the Llama model with appropriate settings
llm = Llama(
    model_path=model_path,
    seed=42,  # Fixed seed for reproducibility
    n_ctx=4096,  # Set the desired context size here
    use_mmap=True,  # Memory mapping for efficiency
    use_mlock=True,  # Prevent swapping to disk for consistent performance
    n_gpu_layers=n_gpu_layers
)

for section, code in parsed_script.items():
    logger.info(f"Processing section: {section}...")

    if section == 'methods' and parsed_script['methods']:
        for method, method_code in parsed_script['methods'].items():
            # Mitigate Code
            new_code_block = run_mitigation_loop(
                bandit_allowance,
                base_name,
                llm,
                logger,
                mitigated_folder,
                mitigated_base_name,
                associated_letter_conversion,
                method_code,
                method)

            # Update code via input section
            new_parsed_script['methods'][method] = new_code_block

    elif section != 'main_script' and parsed_script[section]:
        new_code_block = run_mitigation_loop(
            bandit_allowance,
            base_name,
            llm,
            logger,
            mitigated_folder,
            mitigated_base_name,
            associated_letter_conversion,
            code,
            section)

        # Update code via input section
        new_parsed_script[section] = new_code_block

    else:
        logger.info(f"No code found in section {section}.")

# Define filenames and paths for consolidated and organized code files
consolidated_file_name = f"{mitigated_base_name}_consolidated{ext}"
consolidated_file_path = os.path.join(mitigated_folder, consolidated_file_name)

# Write the consolidated code sections into a single file
with open(consolidated_file_path, 'w') as file:
    # Write global variables section if it exists
    if new_parsed_script['global_variables']:
        file.write(new_parsed_script['global_variables'] + '\n\n')

    # Write methods in alphabetical order
    for method_name in sorted(new_parsed_script['methods']):
        file.write(new_parsed_script['methods'][method_name] + '\n\n')

    # Write main script section if it exists
    if new_parsed_script['main_script']:
        file.write(new_parsed_script['main_script'] + '\n')

# Organize imports and global variables in the consolidated code
organized_fixed_code = organize_imports_and_globals(consolidated_file_path)

# Define filename and path for the organized code file
organized_file_name = f"{mitigated_base_name}_organized{ext}"
organized_file_path = os.path.join(mitigated_folder, organized_file_name)

# Write the organized code to a file
write_to_file(organized_file_path, organized_fixed_code)

# Read the contents of the organized file
with open(organized_file_path, 'r') as file:
    code_content = file.read()

# Use autoimport to fix import issues
fixed_code = autoimport.fix_code(code_content)

# Specify the path to the GGUF model file
model_path = "models/llama-3.2-3b-instruct-q8_0.gguf"

# Retrieve the total number of layers in the model
total_layers = get_block_count_keys(model_path, logger)

# Determine the optimal number of layers to offload to the GPU
n_gpu_layers = get_optimal_gpu_layers(model_path, total_layers)

# Initialize the Llama model with specified settings
llm = Llama(
    model_path=model_path,  # Path to the GGUF model file
    seed=42,  # Fixed seed for reproducibility
    n_ctx=4096,  # Context size for the model
    use_mmap=True,  # Enable memory mapping for efficiency
    use_mlock=True,  # Prevent swapping to disk for consistent performance
    n_gpu_layers=n_gpu_layers  # Number of layers to offload to GPU
)

# Prepare the system prompt for the AI assistant
consolidated_script_system_prompt = (
    f"You are an AI programming assistant proficient in Python application development. You excel at identifying and"
    f" refactoring code to improve security, readability, and efficiency while adhering to best practices. Your "
    f"guidance must be concise, actionable, and prioritize abstraction when reviewing and refactoring the code."
)

# Prepare the user prompt with the code to be reviewed
consolidated_script_user_prompt = (
    f"Please review and refactor the following Python application code. Focus on abstracting redundant logic, "
    f"improving maintainability, and incorporating robust error handling to manage unexpected input types without "
    f"changing the intended behavior or adding any new variables. "
    f"Use concise comments to explain critical modifications:```python\n{fixed_code}\n```"
)


# Combine system and user prompts into a message list
consolidated_script_messages = [
    {"role": "system", "content": consolidated_script_system_prompt},
    {"role": "user", "content": consolidated_script_user_prompt}
]

# Create chat completion llama object
consolidated_llm_config = create_chat_completion_llm(
    llm,
    consolidated_script_messages,
    0.0,
    1,
    0,
    True,
    ["<|endoftext|>"]
)

# Process the streamed output to obtain the adjusted code block
consolidated_code_response = process_streamed_output(consolidated_llm_config)

# Extract the code block from the AI's response
mitigated_code = extract_code_block(consolidated_code_response)

# Define filename and path for the final output file
final_file_name = f"{mitigated_base_name}_final{ext}"
final_mitigated_file_path = os.path.join(mitigated_folder, final_file_name)

# Save the refined code to the final output file
write_to_file(final_mitigated_file_path, mitigated_code)

# Extract the libraries from the mitigated code
extracted_libraries = extract_libraries(mitigated_code)

# Perform scans
continue_boolean = perform_scans(final_mitigated_file_path, associated_letter_conversion, extracted_libraries, logger)

if continue_boolean:
    logger.info(f"Generating unit test file to test {input_file_path} and {final_mitigated_file_path}.")

    # Define filename and path for the final output file
    unit_test_file_name = f"{mitigated_base_name}_unit_test{ext}"
    unit_test_file_path = os.path.join(generated_unit_test_dir, name, associated_letter_conversion, unit_test_file_name)

    # Create the directory for the specialized path for the generated unit test
    os.makedirs(os.path.dirname(unit_test_file_path))

    # Get methods from final mitigated file and generate import statements for the programmatically generated unit test
    method_list = get_methods_from_file(final_mitigated_file_path)
    import_list = [f"\t* {generate_import_statement(final_mitigated_file_path)} import {method}" for method in method_list]
    signatures_list = get_methods_with_signatures(final_mitigated_file_path)
    import_string = '\n'.join(import_list)
    signatures_string = '\n'.join(f"\t* {name}: {signature} -> {rtype}" for name, signature, rtype in signatures_list)

    unit_test_system_prompt = (
        "You are an AI assistant proficient in generating Python unit tests."
    )

    unit_test_user_prompt = (
        "Please generate a minimally viable Python unit test series adhering to the following Mitigated Script, Import Statements and Method Signatures:\n"
        "Mitigated Script:\n"
        f"Import Statements:\n"
        f"{import_string}\n\n"
        f"Method Signatures:\n"
        f"{signatures_string}\n\n"
        "```\n"
        f"\"\"{mitigated_code}\"\"\"\n"
        "```\n\n"
    )

    unit_test_messages = [
        {"role": "system", "content": unit_test_system_prompt},
        {"role": "user", "content": unit_test_user_prompt}
    ]

    unit_test_llm_config = llm.create_chat_completion(
        messages=unit_test_messages,
        temperature=0.0,  # Set temperature for deterministic output
        top_p=1,  # Use nucleus sampling with top_p probability
        top_k=0,  # Disable top-k sampling
        stream=True,  # Enable streaming of the response
        stop=["<|endoftext|>"]  # Define stopping criteria for generation
    )

    # Process the streamed output to obtain the adjusted code block
    unit_test_response = process_streamed_output(unit_test_llm_config)

    # Extract the code block from the AI's response
    unit_test_code = extract_code_block(unit_test_response)

    # Use autoimport to rectify libraries entries
    fixed_unit_test_code = autoimport.fix_code(unit_test_code)

    # Save the refined code to the final output file
    write_to_file(unit_test_file_path, fixed_unit_test_code)

else:
    logger.error(f"Issues still exist with {final_mitigated_file_path}. See above file scan mentions.")
