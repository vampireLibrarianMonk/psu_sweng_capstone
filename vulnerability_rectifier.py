import argparse
import os
import sys
import yaml

import autoimport
from llama_cpp import Llama

from utilities_llm import (setup_logger,
                           get_optimal_gpu_layers,
                           get_block_count_keys,
                           validate_iteration_allowance,
                           parse_python_script, run_mitigation_loop,
                           organize_imports_and_globals, write_to_file, process_streamed_output, extract_code_block,
                           extract_libraries, perform_scans, is_python_file, count_directories_in_path,
                           number_to_excel_column, create_chat_completion_llm, generate_import_statement,
                           get_methods_from_file, get_methods_with_signatures, prepend_to_file, execute_unit_test_file,
                           rename_file, validate_correction_limit, run_method_unit_test_creation_loop
                           )

mitigation_folder = "mitigated_files"

os.makedirs(mitigation_folder, exist_ok=True)

generated_unit_test_dir = "generated_unit_test"

os.makedirs(generated_unit_test_dir, exist_ok=True)

# Specify the path to your GGUF model file
model_path = "models/llama-3.2-3b-instruct-q8_0.gguf"

# Set up argument parser
parser = argparse.ArgumentParser(description='Process a Python file for vulnerability assessment.')
parser.add_argument('file_path', help='Path to the Python file to be analyzed.')
parser.add_argument('iteration_allowance', type=validate_iteration_allowance, help='Allowed number of '
                                                                                'mitigation runs (must be a positive'
                                                                                'integer).')
parser.add_argument('correction_limit', type=validate_correction_limit, help='Allowed number of unit test'
                                                                             'correction mitigation runs (must be a '
                                                                             'positive integer).')

# Parse command-line arguments
args = parser.parse_args()

input_file_path = args.file_path
iteration_allowance = args.iteration_allowance
correction_limit = args.correction_limit

# Get and separate file name into its base name and extension
base_name = os.path.basename(input_file_path)
name, ext = os.path.splitext(base_name)

# Read the specified file and use ast to parse it into a map for quicker analysis and mitigation
parsed_script = parse_python_script(input_file_path)

new_parsed_script = {
    'methods': {}
}

# Get method count of parsed script
method_count = len(parsed_script['methods'].items())

# Define base name for mitigated files
mitigated_base_name = f"{name}_mitigated"

# Get the file count
file_count = count_directories_in_path(os.path.join(mitigation_folder, mitigated_base_name))

# Create folder for mitigated files
associated_letter_conversion = number_to_excel_column(file_count)
mitigated_folder = os.path.join(mitigation_folder, mitigated_base_name, associated_letter_conversion)

# Create the mitigated folder associated with the letter conversion
os.makedirs(mitigated_folder, exist_ok=True)

# Initialize the logger
logger = setup_logger(f"vulnerability_rectifier", associated_letter_conversion, name)

# Load the YAML file
prompt_yaml = "config/prompt.yaml"
logger.info(f"Loading prompt configurations from {prompt_yaml}")
with open(prompt_yaml, 'r') as file:
    data = yaml.safe_load(file)

# Convert the prompts to a dictionary keyed by role
prompts_dict = {prompt['role']: prompt['content'] for prompt in data['prompts']}

# Extract the prompts directly into variables
mitigation_system_prompt = prompts_dict['mitigation_system_prompt']
mitigation_user_prompt = prompts_dict['mitigation_user_prompt']
refactor_script_system_prompt = prompts_dict['refactor_script_system_prompt']
refactor_script_user_prompt = prompts_dict['refactor_script_user_prompt']

# Vet the input file as a python script file
if is_python_file(input_file_path):
    with open(input_file_path, 'r') as file:
        original_code = file.read()
else:
    logger.error(f"{input_file_path} is not a valid Python file.")
    sys.exit(1)

# Alternative way for getting block count using Llama
# Load the model
# llm = Llama(model_path=model_path)

# Access the model's metadata
# metadata = llm.metadata

# Retrieve the block count
# total_layers = int(metadata.get('llama.block_count'))

# Get the total layers from the input model
total_layers = get_block_count_keys(model_path, logger)

# Determine the optimal number of GPU layers to offload
n_gpu_layers = get_optimal_gpu_layers(model_path, total_layers)

# Initialize the Llama model with appropriate settings
llm = Llama(
    model_path=model_path,
    seed=42,  # Fixed seed for reproducibility
    n_ctx=4096, #Set the desired context size here
    use_mmap=True,  # Memory mapping for efficiency
    use_mlock=True,  # Prevent swapping to disk for consistent performance
    n_gpu_layers=n_gpu_layers
)

# Only process the methods that
if parsed_script['methods']:
    logger.info(f"Processing parsed methods:")
    for method, method_code in parsed_script['methods'].items():
        # Mitigate Code
        new_code_block = run_mitigation_loop(
            mitigation_system_prompt,
            mitigation_user_prompt,
            iteration_allowance,
            base_name,
            llm,
            logger,
            mitigated_folder,
            mitigated_base_name,
            associated_letter_conversion,
            method_code,
            method)

        # Update code via input section
        new_parsed_script['methods'][method] = new_code_block

# Define filenames and paths for consolidated and organized code files
consolidated_file_name = f"{mitigated_base_name}_consolidated{ext}"
consolidated_file_path = os.path.join(mitigated_folder, consolidated_file_name)

# Write the consolidated code sections into a single file
with open(consolidated_file_path, 'w') as file:
    # Write methods in alphabetical order
    for method_name in sorted(new_parsed_script['methods']):
        file.write(new_parsed_script['methods'][method_name] + '\n\n')

# Organize imports and global variables in the consolidated code
organized_fixed_code = organize_imports_and_globals(consolidated_file_path)

# Define filename and path for the organized code file
organized_file_name = f"{mitigated_base_name}_organized{ext}"
organized_file_path = os.path.join(mitigated_folder, organized_file_name)

# Write the organized code to a file
write_to_file(organized_file_path, organized_fixed_code)

# Read the contents of the organized file
with open(organized_file_path, 'r') as file:
    code_content = file.read()

# Use autoimport to fix import issues
fixed_code = autoimport.fix_code(code_content)

# Combine system and user prompts into a message list
refactor_script_user_prompt = refactor_script_user_prompt.format(fixed_code=fixed_code)
refactor_script_messages = [
    {"role": "system", "content": refactor_script_system_prompt},
    {"role": "user", "content": refactor_script_user_prompt}
]

# Create chat completion llama object
refactor_llm_config = create_chat_completion_llm(
    llm,
    refactor_script_messages,
    0.0,
    1,
    0,
    True,
    ["<|endoftext|>"]
)

# Process the streamed output to obtain the adjusted code block
refactor_code_response = process_streamed_output(refactor_llm_config)

# Extract the code block from the AI's response
refactored_code = extract_code_block(refactor_code_response)

# Use autoimport to fix any dangling imports
refactored_code = autoimport.fix_code(refactored_code)

# Define filename and path for the final output file
refactored_file_name = f"{mitigated_base_name}_refactored{ext}"
refactored_file_path = os.path.join(mitigated_folder, refactored_file_name)

# Save the refined code to the final output file
write_to_file(refactored_file_path, refactored_code)

# Perform the final mitigation of the refactored, organized and consolidated code
logger.info(f"Starting final mitigation of refactored code {refactored_file_path}.")
final_mitigated_code_block = run_mitigation_loop(
    mitigation_system_prompt,
    mitigation_user_prompt,
    iteration_allowance,
    base_name,
    llm,
    logger,
    mitigated_folder,
    mitigated_base_name,
    associated_letter_conversion,
    refactored_code,
    "refactored")

# Define filename and path for the final output file
final_file_name = f"{mitigated_base_name}_final{ext}"
final_file_path = os.path.join(mitigated_folder, final_file_name)

# Save the refined code to the final output file
write_to_file(final_file_path, final_mitigated_code_block)

#     # Get methods from final mitigated file and generate import statements for the programmatically generated unit test
#     method_list = get_methods_from_file(final_mitigated_file_path)
#     methods_string = "\n".join(method_list)
#     import_list = [f"{generate_import_statement(final_mitigated_file_path)} import {method}" for method in method_list]
#     import_string = '\n'.join(import_list)
#     # signatures_list = get_methods_with_signatures(final_mitigated_file_path)
#     # signatures_string = '\n'.join(f"{name}: {signature} -> {rtype}" for name, signature, rtype in signatures_list)

# if vulnerability_gate:
#     logger.info(f"Generating unit test file to test {input_file_path} and {final_mitigated_file_path}.")
#
#     # Read the specified file and use ast to parse it into a map for quicker analysis and mitigation
#     mitigated_parsed_script = parse_python_script(final_mitigated_file_path)
#
#     unit_test_parsed_script = {
#         'global_variables': '',
#         'methods': {},
#         'main_script': ''
#     }
#
#     for section, code in mitigated_parsed_script.items():
#         logger.info(f"Processing section: {section}...")
#
#         os.makedirs(os.path.join(generated_unit_test_dir, associated_letter_conversion), exist_ok=True)
#
#         if section == 'methods' and mitigated_parsed_script['methods']:
#             for method, method_code in mitigated_parsed_script['methods'].items():
#                 # Mitigate Code
#                 new_code_block = run_method_unit_test_creation_loop(
#                     correction_limit,
#                     base_name,
#                     llm,
#                     logger,
#                     generated_unit_test_dir,
#                     mitigated_base_name,
#                     associated_letter_conversion,
#                     method_code,
#                     method,
#                 f"{generate_import_statement(final_mitigated_file_path)} import {method}")
#
#                 # Update code via input section
#                 unit_test_parsed_script['methods'][method] = new_code_block
#
#         elif section != 'main_script' and mitigated_parsed_script[section]:
#             pass
#
#         else:
#             logger.info(f"No code found in section {section}.")