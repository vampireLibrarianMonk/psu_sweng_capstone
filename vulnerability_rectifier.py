import argparse
import os

import autoimport
from llama_cpp import Llama

from utilities_llm import (setup_logger,
                           get_optimal_gpu_layers,
                           get_block_count_keys,
                           validate_bandit_allowance,
                           parse_python_script, run_mitigation_loop,
                           organize_imports_and_globals, write_to_file, process_streamed_output, extract_code_block,
                           analyze_file_with_semgrep, get_semgrep_issues, extract_libraries, get_dodgy_issues,
                           analyze_file_with_dodgy, analyze_file_with_bandit, get_bandit_issues
                           )

mitigated_folder = "mitigated_files"

os.makedirs(mitigated_folder, exist_ok=True)

# Specify the path to your GGUF model file
model_path = "models/llama-3.2-3b-instruct-q8_0.gguf"

# Set up argument parser
parser = argparse.ArgumentParser(description='Process a Python file for vulnerability assessment.')
parser.add_argument('file_path', help='Path to the Python file to be analyzed.')
parser.add_argument('bandit_allowance', type=validate_bandit_allowance, help='Allowed number of Bandit '
                                                                             'mitigation runs (must be a positive '
                                                                             'integer).')

# Parse command-line arguments
args = parser.parse_args()

input_file_path = args.file_path
bandit_allowance = args.bandit_allowance

# Get and separate file name into its base name and extension
base_name = os.path.basename(input_file_path)
name, ext = os.path.splitext(base_name)

# Initialize the logger
logger = setup_logger(f"vulnerability_rectifier_{name}")

# Read the specified file and use ast to parse it into a map for quicker analysis and mitigation
parsed_script = parse_python_script(input_file_path)

# Get the total layers from the input model
total_layers = get_block_count_keys(model_path, logger)

# Determine the optimal number of GPU layers to offload
n_gpu_layers = get_optimal_gpu_layers(model_path, total_layers)

# Initialize the Llama model with appropriate settings
llm = Llama(
    model_path=model_path,
    seed=42,  # Fixed seed for reproducibility
    n_ctx=4096,  # Set the desired context size here
    use_mmap=True,  # Memory mapping for efficiency
    use_mlock=True,  # Prevent swapping to disk for consistent performance
    n_gpu_layers=n_gpu_layers
)

new_parsed_script = {
    'global_variables': '',
    'methods': {},
    'main_script': ''
}

# Get method count of parsed script
method_count = len(parsed_script['methods'].items())

# Define base name for mitigated files
mitigated_base_name = f"{name}_mitigated"

# Define filename and path for the final output file
final_file_name = f"{mitigated_base_name}_final{ext}"
final_file_path = os.path.join(mitigated_folder, final_file_name)

# Only move forward with more complex processing if the method count is more than 3
if method_count < 4:
    # Read the contents of the organized file
    with open(input_file_path, 'r') as file:
        original_code = file.read()

    new_code_block = run_mitigation_loop(
                bandit_allowance,
                base_name,
                llm,
                logger,
                mitigated_folder,
                original_code,
                'simple')

    # Save the refined code to the final output file
    if new_code_block:

        # Use autoimport to fix import issues
        fixed_code = autoimport.fix_code(new_code_block)

        write_to_file(final_file_path, fixed_code)
    else:
        logger.info("No code generated to write.")
else:
    for section, original_code in parsed_script.items():
        logger.info(f"Processing section: {section}...")

        if section == 'methods' and parsed_script['methods']:
            for method, method_code in parsed_script['methods'].items():
                # Mitigate Code
                new_code_block = run_mitigation_loop(
                    bandit_allowance,
                    base_name,
                    llm,
                    logger,
                    mitigated_folder,
                    method_code,
                    method)

                # Update code via input section
                new_parsed_script['methods'][method] = new_code_block

        elif section != 'main_script' and parsed_script[section]:
            new_code_block = run_mitigation_loop(
                bandit_allowance,
                base_name,
                llm,
                logger,
                mitigated_folder,
                original_code,
                section)

            # Update code via input section
            new_parsed_script[section] = new_code_block

        else:
            logger.info(f"No code found in section {section}.")

    # Define filenames and paths for consolidated and organized code files
    consolidated_file_name = f"{mitigated_base_name}_consolidated{ext}"
    consolidated_file_path = os.path.join(mitigated_folder, consolidated_file_name)

    # Write the consolidated code sections into a single file
    with open(consolidated_file_path, 'w') as file:
        # Write global variables section if it exists
        if new_parsed_script['global_variables']:
            file.write(new_parsed_script['global_variables'] + '\n\n')

        # Write methods in alphabetical order
        for method_name in sorted(new_parsed_script['methods']):
            file.write(new_parsed_script['methods'][method_name] + '\n\n')

        # Write main script section if it exists
        if new_parsed_script['main_script']:
            file.write(new_parsed_script['main_script'] + '\n')

    # Organize imports and global variables in the consolidated code
    organized_fixed_code = organize_imports_and_globals(consolidated_file_path)

    # Define filename and path for the organized code file
    organized_file_name = f"{mitigated_base_name}_organized{ext}"
    organized_file_path = os.path.join(mitigated_folder, organized_file_name)

    # Write the organized code to a file
    write_to_file(organized_file_path, organized_fixed_code)

    # Read the contents of the organized file
    with open(organized_file_path, 'r') as file:
        code_content = file.read()

    # Use autoimport to fix import issues
    fixed_code = autoimport.fix_code(code_content)

    # Specify the path to the GGUF model file
    model_path = "models/llama-3.2-3b-instruct-q8_0.gguf"

    # Retrieve the total number of layers in the model
    total_layers = get_block_count_keys(model_path, logger)

    # Determine the optimal number of layers to offload to the GPU
    n_gpu_layers = get_optimal_gpu_layers(model_path, total_layers)

    # Initialize the Llama model with specified settings
    llm = Llama(
        model_path=model_path,  # Path to the GGUF model file
        seed=42,  # Fixed seed for reproducibility
        n_ctx=4096,  # Context size for the model
        use_mmap=True,  # Enable memory mapping for efficiency
        use_mlock=True,  # Prevent swapping to disk for consistent performance
        n_gpu_layers=n_gpu_layers  # Number of layers to offload to GPU
    )

    # Prepare the system prompt for the AI assistant
    system_prompt = (
        f"You are an AI programming assistant proficient in Python application development. You excel at identifying and"
        f" refactoring code to improve security, readability, and efficiency while adhering to best practices. Your "
        f"guidance must be concise, actionable, and prioritize abstraction when reviewing and refactoring the code."
    )

    # Prepare the user prompt with the code to be reviewed
    user_prompt = (
        f"Please review and refactor the following Python application code. Focus on abstracting redundant logic and "
        f"improving maintainability without changing the intended behavior or adding any new variables. "
        f"Use concise comments to explain critical modifications:```python\n{fixed_code}\n```"
    )

    # Combine system and user prompts into a message list
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    # Generate code with adjusted parameters
    """
    Parameters for adjustment
    ----------
    temperature : float, optional
        Controls the randomness of the output, with a range from 0 to 2. Lower values (e.g., 0.2) produce more 
        focused and deterministic responses, while higher values (e.g., 0.8) yield more varied and creative outputs.
         It is generally recommended to adjust either `temperature` or `top_p`, but not both simultaneously. 
    
    top_p : float, optional
        Also known as nucleus sampling, this parameter ranges from 0 to 1 and determines the diversity of the output
         by considering only the tokens that comprise the top `p` probability mass. For instance, a `top_p` of 0.1 
         means only the tokens within the top 10% probability mass are considered. Adjusting `top_p` can influence 
         the creativity of the response, with lower values leading to more focused outputs.
    
    top_k : int, optional
        Limits the next token selection to the top `k` tokens with the highest probabilities. Setting `top_k` to 0 
        effectively disables this filtering, allowing the model to consider all possible tokens. Adjusting `top_k` 
        can control the diversity of the output, with lower values leading to more focused and deterministic 
        responses.
    """
    response = llm.create_chat_completion(
        messages=messages,
        temperature=0.0,  # Set temperature for deterministic output
        top_p=1,  # Use nucleus sampling with top_p probability
        top_k=0,  # Disable top-k sampling
        stream=True,  # Enable streaming of the response
        stop=["<|endoftext|>"]  # Define stopping criteria for generation
    )

    # Process the streamed output to obtain the adjusted code block
    adjusted_code_block = process_streamed_output(response)

    # Extract the code block from the AI's response
    extracted_code_block = extract_code_block(adjusted_code_block)

    # Save the refined code to the final output file
    write_to_file(final_file_path, extracted_code_block)

    # Perform scans again
    logger.info(f"Performing scans on {final_file_path}")

    # Bandit
    bandit_scan_json_path = analyze_file_with_bandit(final_file_path, logger)
    bandit_issues = get_bandit_issues(bandit_scan_json_path)

    if bandit_issues == '':
        logger.info("No issues encountered after bandit scan.")
    else:
        logger.info("The following was found after bandit scan:")
        logger.info(bandit_issues)

    # Dodgy
    dodgy_scan_json_path = analyze_file_with_dodgy(final_file_path, logger)
    dodgy_issues = get_dodgy_issues(dodgy_scan_json_path)

    if dodgy_issues == 'No issues found.':
        logger.info("No issues encountered after dodgy scan.")
    else:
        logger.info("The following was found after dodgy scan:")
        logger.info(dodgy_issues)

    # Semgrep
    # Extract the libraries from the mitigated code
    extracted_libraries = extract_libraries(extracted_code_block)

    semgrep_scan_json_path = analyze_file_with_semgrep(final_file_path, extracted_libraries, logger)
    semgrep_issues = get_semgrep_issues(semgrep_scan_json_path)

    if semgrep_issues == 'No issues found.':
        logger.info("No issues encountered after semgrep scan.")
    else:
        logger.info("The following was found after semgrep scan:")
        logger.info(semgrep_issues)