import argparse
import json
import os
import re

import tiktoken
from llama_cpp import Llama

from utilities import analyze_file_with_bandit, setup_logger, get_optimal_gpu_layers, get_block_count_keys, \
    validate_bandit_allowance, extract_module_names

# Initialize the logger
logger = setup_logger(f"vulnerability_detector")

# Set up argument parser
parser = argparse.ArgumentParser(description='Process a Python file for vulnerability assessment.')
parser.add_argument('file_path', help='Path to the Python file to be analyzed.')
parser.add_argument('bandit_allowance', help='Allowed iteration of bandit mitigation runs (inclusive).')

# Parse command-line arguments
args = parser.parse_args()

# Set up argument parser
parser = argparse.ArgumentParser(description='Process a Python file for vulnerability assessment.')
parser.add_argument('file_path', help='Path to the Python file to be analyzed.')
parser.add_argument('bandit_allowance', type=validate_bandit_allowance, help='Allowed number of Bandit mitigation runs (must be a positive integer).')

# Parse command-line arguments
args = parser.parse_args()

input_file_path = args.file_path
bandit_allowance = args.bandit_allowance

# Get and separate file name into its base name and extension
base_name = os.path.basename(input_file_path)
name, ext = os.path.splitext(base_name)

# Read the specified file
try:
    with open(input_file_path, 'r') as file:
        original_code = file.read()
        logger.info(f"Successfully read file: {input_file_path}")
except FileNotFoundError:
    logger.error(f"The file '{input_file_path}' was not found.")
    exit(1)
except IOError as e:
    logger.error(f"An error occurred while reading the file '{input_file_path}': {e}")
    exit(1)


def extract_code_block(text):
    """
    Extracts the first Python code block from the given text.

    Args:
        text (str): The input text containing code blocks.

    Returns:
        str: The extracted code block, or None if no code block is found.
    """
    match = re.search(r'```python\n(.*?)```', text, re.DOTALL)
    return match.group(1) if match else None


def extract_secure_implementation_title(text):
    pattern = r"\*\*Secure Implementation Title:?\*\*:?(.*?)\n"
    match = re.search(pattern, text, re.DOTALL)
    return match.group(1).strip().replace("\n", "") if match else None


def extract_secure_implementation_statement(text):
    pattern = r"\*\*Secure Implementation Statement:?\*\*:?(.*?)\n+"
    match = re.search(pattern, text, re.DOTALL)
    return match.group(1).strip().replace("\n", "") if match else None


def extract_secure_implementation_explanation_statement(text):
    pattern = r"\*\*Secure Code Implementation Statement:?\*\*:?(.*)\n*"
    match = re.search(pattern, text, re.DOTALL)
    return match.group(1).strip().replace("\n", "") if match else None


def get_bandit_issues(input_bandit_file):
    # Open and read the JSON file
    with open(input_bandit_file, 'r') as file:
        data = json.load(file)

    # Map test_id to (line_number, line_range, issue_text)
    test_id_mapping_with_range = {
        result["test_id"]: (result["line_number"], result["line_range"], result["issue_text"])
        for result in data["results"]
    }

    # Construct Bandit issues for user prompt
    bandit_issues = "\n".join(
        f"- Test ID: {test_id}, Line: {details[0]}, Line Range: {details[1]}, Issue: {details[2]}"
        for test_id, details in test_id_mapping_with_range.items()
    )

    return bandit_issues


def process_streamed_output(response, print_stream= False):
    """
    Processes the streamed output chunk by chunk.

    Args:
        response (iterator): An iterator yielding chunks of response content.
        print_stream: Boolean for printing what is in the stream as it goes.

    Returns:
        str: The complete output from the stream.
    """
    complete_output = ""
    for chunk in response:
        content = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
        if print_stream:
            print(content, end="", flush=True)  # Print in real-time
        complete_output += content
    logger.info("\n")  # Ensure a new line after the streamed output
    return complete_output


def save_code_to_file(code, path):
    """
    Saves the given code to a specified file.

    Args:
        code (str): The code to be saved.
        path (str): The path to save the code in.
    """
    with open(path, 'w') as file_io:
        file_io.write(code)
    logger.info(f"Code saved to file: {path}")


mitigated_folder = "mitigated_files"

os.makedirs(mitigated_folder, exist_ok=True)
logger.info(f"Ensured mitigated folder exists: {mitigated_folder}")

# Specify the path to your GGUF model file
model_path = "models/llama-3.2-3b-instruct-q8_0.gguf"
logger.info(f"Model path set to: {model_path}")

total_layers = get_block_count_keys(model_path, logger)

# Determine the optimal number of GPU layers to offload
n_gpu_layers = get_optimal_gpu_layers(model_path, total_layers)

# Initialize the Llama model with appropriate settings
llm = Llama(
    model_path=model_path,
    seed=42,  # Fixed seed for reproducibility
    n_ctx=4096,  # Set the desired context size here
    use_mmap=True,  # Memory mapping for efficiency
    use_mlock=True,  # Prevent swapping to disk for consistent performance
    n_gpu_layers=n_gpu_layers
)

# Define the system and user prompts
system_prompt = (
    "A chat between a developer and an artificial intelligence programming assistant fluent in secure programming techniques. "
    "The assistant gives a single concise, secure, detailed and polite answers to the user's questions."
)

user_prompt = (
    "Provide a single, concise sentence suggesting the best way to fix this Python code. The suggestion should "
    "focus on replacing unsafe practices with secure alternatives without describing specific methods or "
    "implementations.\n\n"
    f"```python\n{original_code}\n```"
)

# Create the chat messages sequence
messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_prompt}
]

# Initialize the tokenizer for GGUF models
tokenizer = tiktoken.get_encoding("cl100k_base")

# Calculate the number of tokens in the user prompt
input_tokens = len(tokenizer.encode(system_prompt + user_prompt))
logger.debug(f"Input tokens: {input_tokens}")

# Dynamically calculate max_tokens (e.g., 4x input tokens, capped at 4096)
max_tokens = min(input_tokens * 2, 512)

# Generate the assessment using chat completion
assessment_response = llm.create_chat_completion(
    messages=messages,
    max_tokens=max_tokens,
    temperature=0.0,
    top_p=1.0,
    top_k=0,
    stop=["<|endoftext|>"]  # Stop generation at the EOS token
)

# Extract and process the assessment
assessment = assessment_response['choices'][0]['message']['content'].strip()
logger.info(f"Vulnerability Assessment: {assessment}")

# Define the system and user prompts
secure_system_assessment_prompt = (
    "You are an AI programming assistant specializing in secure coding practices."
)

# Generate secure code suggestion
secure_user_assessmment_prompt = (
    f"Based on the identified vulnerability assessment {assessment}, provide a write-up. "
    "Your responses should include: '**Secure Implementation Title**', '**Secure Implementation Statement**', '**Secure Code Implementation Statement**'."
    "Ensure all recommendations adhere to best security practices and avoid including any code examples."
)

messages = [
    {"role": "system", "content": secure_system_assessment_prompt},
    {"role": "user", "content": secure_user_assessmment_prompt}
]

input_tokens = len(tokenizer.encode(secure_system_assessment_prompt + secure_user_assessmment_prompt))
max_tokens = min(input_tokens * 8, 2048)

secure_code_assessment_response = llm.create_chat_completion(
    messages=messages,
    max_tokens=max_tokens,
    temperature=0.0,
    top_p=1.0,
    top_k=0,
    stop=["<|endoftext|>"]  # Stop generation at the EOS token
)

secure_code_assessment = secure_code_assessment_response['choices'][0]['message']['content'].strip()
secure_title = extract_secure_implementation_title(secure_code_assessment)
secure_implementation_statement = extract_secure_implementation_statement(secure_code_assessment)
secure_implementation_explanation_statement = extract_secure_implementation_explanation_statement(secure_code_assessment)
logger.info(f"Secure Implementation Title: {secure_title}")
logger.info(f"Secure Implementation Statement: {secure_implementation_statement}")
logger.info(f"Secure Implementation Explanation Statement: {secure_implementation_explanation_statement}")

# Define the system and user prompts
secure_system_code_prompt = (
    "You are an AI programming assistant specializing in secure coding practices."
)

# Generate secure code suggestion with explanations provided exclusively as docstrings or inline comments
line_count = int(len(original_code.split("\n")) * 1.20)
word_count = len(original_code.split(" "))

secure_user_code_prompt = (
    f"Based on the identified vulnerability assessment {assessment}, provide a secure version of the following Python "
    "code.\n"
    f"Ensure the code does not exceed {word_count} words.\n"
    f"Ensure the code does not exceed {line_count} lines.\n"
    "Only write the code, docstrings and inline comments.\n\n"
    f"```python\n{original_code}\n```"
)

messages = [
    {"role": "system", "content": secure_system_code_prompt},
    {"role": "user", "content": secure_user_code_prompt}
]

secure_code_response = llm.create_chat_completion(
    messages=messages,
    max_tokens=int(len(original_code.split(" ")) * 4),
    temperature=0.0,
    top_p=1.0,
    top_k=0,
    stop=["<|endoftext|>"]  # Stop generation at the EOS token
)

secure_code_writeup = secure_code_response['choices'][0]['message']['content'].strip()

# Extract the code block from the secure code suggestion
code_block = extract_code_block(secure_code_writeup)

# Load the module substitution JSON file
with open('substitution/module.json', 'r') as file:
    module_associations = json.load(file)
    if not isinstance(module_associations, dict):
        raise ValueError("The loaded substitutions JSON data is not a dictionary.")

# Set initial and maximum temperature values
initial_temperature = 0.0
max_temperature = 2.0

# Calculate increment per iteration
temperature_increment = (max_temperature - initial_temperature) / bandit_allowance

# Define initial constraints based on the current mitigated code
initial_line_count = len(code_block.split("\n"))
initial_word_count = len(code_block.split())

if code_block:
    iteration = 0
    name, ext = os.path.splitext(base_name)
    mitigated_file_name = f"{name}_mitigated_iteration_{iteration}{ext}"
    mitigated_file_path = os.path.join(mitigated_folder, mitigated_file_name)
    save_code_to_file(code_block, mitigated_file_path)

    bandit_scan_json_path = analyze_file_with_bandit(mitigated_file_path, logger)

    bandit_issues = get_bandit_issues(bandit_scan_json_path)

    if bandit_issues == '':
        logger.info(f"Stopping mitigation loop as no bandit issues were encountered for {mitigated_file_name}.")

    logger.info(f"Running an iteration series of {bandit_allowance}.")
    while bandit_issues != '' and iteration < bandit_allowance:
        logger.info(f"Running iteration {iteration}.")

        # Read the specified file
        try:
            mitigated_file_name = f"{name}_mitigated_iteration_{iteration}{ext}"
            mitigated_file_path = os.path.join(mitigated_folder, mitigated_file_name)
            with open(mitigated_file_path, 'r') as file:
                mitigated_code = file.read()
                logger.info(f"Successfully read file: {mitigated_file_path}")
        except FileNotFoundError:
            logger.error(f"The file '{mitigated_file_path}' was not found.")
            exit(1)
        except IOError as e:
            logger.error(f"An error occurred while reading the file '{mitigated_file_path}': {e}")
            exit(1)

        # Define the system and user prompts
        bandit_system_assessment_prompt = (
            "You are an AI programming assistant specializing in secure coding practices."
        )

        bandit_user_assessment_prompt = (
            f"This is iteration {iteration} of this specific request.\n"
            f"{'Immediate action is required to resolve these issues.' if iteration > 1 else ''} "
            f"The Bandit static analysis tool has identified the following issues in the provided Python code:\n"
            f"{bandit_issues}\n\n"
            f"Please provide a concise write-up addressing these specific issues. "
            f"Your response should include: '**Secure Implementation Title**', '**Secure Implementation Statement**', "
            f"and '**Secure Code Implementation Statement**'. "
            f"Ensure all recommendations adhere to best security practices and avoid including any code examples. "
        )

        messages = [
            {"role": "system", "content": bandit_system_assessment_prompt},
            {"role": "user", "content": bandit_user_assessment_prompt}
        ]

        # Create a chat completion request with streaming enabled
        logger.info("Starting secure code generation with streaming.")
        bandit_assessment_response = llm.create_chat_completion(
            messages=messages,
            max_tokens=max_tokens,
            temperature=0.0,
            top_p=1.0,
            top_k=0,
            stream=True,  # Enable streaming
            stop=["<|endoftext|>"]  # Stop generation at the EOS token
        )

        # Process the streamed response
        bandit_assessment = process_streamed_output(bandit_assessment_response)
        logger.info("Bandit code assessment completed.")

        # Extract additional components from the streamed response
        adjusted_secure_title = extract_secure_implementation_title(bandit_assessment)
        adjusted_secure_implementation_statement = extract_secure_implementation_statement(bandit_assessment)
        adjusted_secure_implementation_explanation_statement = extract_secure_implementation_explanation_statement(
            bandit_assessment)
        logger.info(f"Adjusted Secure Implementation Title: {adjusted_secure_title}")
        logger.info(f"Adjusted Secure Implementation Statement: {adjusted_secure_implementation_statement}")
        logger.info(
            f"Adjusted Secure Implementation Explanation Statement: {adjusted_secure_implementation_explanation_statement}")

        # Define the system and user prompts
        bandit_system_prompt = (
            "You are an AI programming assistant specializing in secure coding practices."
        )

        # Calculate increments per iteration
        line_count_increment = initial_line_count * 0.5 / bandit_allowance
        word_count_increment = initial_word_count * 0.5 / bandit_allowance

        # Adjust constraints dynamically based on the current iteration
        line_count = int(initial_line_count * 1.1 + line_count_increment * iteration)
        word_count = int(initial_word_count * 1.2 + word_count_increment * iteration)

        # Construct the substitution part of the prompt
        if module_associations:
            substitution_statements = '\n'.join(
                [f"\t* For '{vuln_module}', {guidance}" for vuln_module, guidance in module_associations.items()]
            )
            substitution_instruction = (
                f"Please provide an adjusted secure version of the code below, addressing the following issues:\n"
                f"{substitution_statements}\n"
            )
        else:
            substitution_instruction = (
                "Please provide an adjusted secure version of the code below, addressing the identified issues.\n"
            )

        # Construct the complete prompt
        bandit_code_prompt = (
            f"{adjusted_secure_title}\n"
            f"{adjusted_secure_implementation_statement}\n"
            f"{adjusted_secure_implementation_explanation_statement}\n\n"
            f"The Bandit static analysis tool has identified the following issues in the provided Python code:\n"
            f"{bandit_issues}\n\n"
            f"{substitution_instruction}\n"
            f"Ensure that all recommendations adhere to best security practices.\n"
            f"Ensure the code does not exceed {word_count} words.\n"
            f"Ensure the code does not exceed {line_count} lines.\n\n"
            f"Only include the code, along with appropriate docstrings and inline comments:\n\n"
            f"```python\n{mitigated_code}\n```"
        )

        messages = [
            {"role": "system", "content": bandit_system_prompt},
            {"role": "user", "content": bandit_code_prompt}
        ]

        # Calculate decrement per iteration
        initial_temperature = 0.0
        temperature_decrement = initial_temperature / bandit_allowance

        # Adjust temperature to control randomness; increase to make output more diverse
        temperature = min(max_temperature, initial_temperature + temperature_increment * iteration)

        # Generate code with adjusted parameters
        """
        Parameters for adjustment
        ----------
        temperature : float, optional
            Controls the randomness of the output, with a range from 0 to 2. Lower values (e.g., 0.2) produce more focused and deterministic responses, while higher values (e.g., 0.8) yield more varied and creative outputs. It is generally recommended to adjust either `temperature` or `top_p`, but not both simultaneously. [Source: OpenAI API Documentation]

        top_p : float, optional
            Also known as nucleus sampling, this parameter ranges from 0 to 1 and determines the diversity of the output by considering only the tokens that comprise the top `p` probability mass. For instance, a `top_p` of 0.1 means only the tokens within the top 10% probability mass are considered. Adjusting `top_p` can influence the creativity of the response, with lower values leading to more focused outputs. [Source: OpenAI API Documentation]

        top_k : int, optional
            Limits the next token selection to the top `k` tokens with the highest probabilities. Setting `top_k` to 0 effectively disables this filtering, allowing the model to consider all possible tokens. Adjusting `top_k` can control the diversity of the output, with lower values leading to more focused and deterministic responses. [Source: OpenAI API Documentation]
        """
        logger.info(f"Using an adjusted temperature of {temperature}.")
        bandit_adjusted_code_response = llm.create_chat_completion(
            messages=messages, # system and user prompt
            temperature=temperature,
            top_p=1,
            top_k=0,
            stream=True,  # Enable streaming
            stop=["<|endoftext|>"]  # Stop generation at the end-of-text token
        )

        # Process the streamed response
        adjusted_code_block = process_streamed_output(bandit_adjusted_code_response)
        logger.info("Secure code generation completed.")

        # Extract the code block from the secure code suggestion
        code_block = extract_code_block(adjusted_code_block)

        if code_block:
            name, ext = os.path.splitext(base_name)
            iteration += 1
            mitigated_file_name = f"{name}_mitigated_iteration_{iteration}{ext}"
            mitigated_file_path = os.path.join(mitigated_folder, mitigated_file_name)
            save_code_to_file(code_block, mitigated_file_path)
        else:
            logger.warning("No adjusted code block found in the secure code suggestion. Stopping to diagnose issue.")
            break

        bandit_scan_json_path = analyze_file_with_bandit(mitigated_file_path, logger)

        bandit_issues = get_bandit_issues(bandit_scan_json_path)

else:
    logger.warning("No code block found in the secure code suggestion.")
