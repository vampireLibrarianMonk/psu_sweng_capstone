import argparse
import os
import sys

import autoimport
from llama_cpp import Llama

from utilities_llm import (setup_logger,
                           get_optimal_gpu_layers,
                           get_block_count_keys,
                           validate_bandit_allowance,
                           parse_python_script, run_mitigation_loop,
                           organize_imports_and_globals, write_to_file, process_streamed_output, extract_code_block,
                           extract_libraries, perform_scans, is_python_file, count_directories_in_path,
                           number_to_excel_column, create_chat_completion_llm
                           )

mitigation_folder = "mitigated_files"

os.makedirs(mitigation_folder, exist_ok=True)

generated_unit_test_dir = "generated_unit_test"

os.makedirs(generated_unit_test_dir, exist_ok=True)

# Specify the path to your GGUF model file
model_path = "models/llama-3.2-3b-instruct-q8_0.gguf"

# Set up argument parser
parser = argparse.ArgumentParser(description='Process a Python file for vulnerability assessment.')
parser.add_argument('file_path', help='Path to the Python file to be analyzed.')
parser.add_argument('bandit_allowance', type=validate_bandit_allowance, help='Allowed number of Bandit '
                                                                             'mitigation runs (must be a positive '
                                                                             'integer).')

# Parse command-line arguments
args = parser.parse_args()

input_file_path = args.file_path
bandit_allowance = args.bandit_allowance

# Vet the input file as a python script file
if is_python_file(input_file_path):
    with open(input_file_path, 'r') as file:
        original_code = file.read()
else:
    print(f"{input_file_path} is not a valid Python file.")
    sys.exit(1)

# Get and separate file name into its base name and extension
base_name = os.path.basename(input_file_path)
name, ext = os.path.splitext(base_name)

# Read the specified file and use ast to parse it into a map for quicker analysis and mitigation
parsed_script = parse_python_script(input_file_path)

new_parsed_script = {
    'global_variables': '',
    'methods': {},
    'main_script': ''
}

# Get method count of parsed script
method_count = len(parsed_script['methods'].items())

# Define base name for mitigated files
mitigated_base_name = f"{name}_mitigated"

# Get the file count
file_count = count_directories_in_path(os.path.join(mitigation_folder, mitigated_base_name))

# Create folder for mitigated files
associated_letter_conversion = number_to_excel_column(file_count)
mitigated_folder = os.path.join(mitigation_folder, mitigated_base_name, associated_letter_conversion)

# Initialize the logger
logger = setup_logger(f"vulnerability_rectifier", associated_letter_conversion, name)

os.makedirs(mitigated_folder, exist_ok=True)

# Get the total layers from the input model
total_layers = get_block_count_keys(model_path, logger)

# Determine the optimal number of GPU layers to offload
n_gpu_layers = get_optimal_gpu_layers(model_path, total_layers)

# Initialize the Llama model with appropriate settings
llm = Llama(
    model_path=model_path,
    seed=42,  # Fixed seed for reproducibility
    n_ctx=4096,  # Set the desired context size here
    use_mmap=True,  # Memory mapping for efficiency
    use_mlock=True,  # Prevent swapping to disk for consistent performance
    n_gpu_layers=n_gpu_layers
)

for section, code in parsed_script.items():
    logger.info(f"Processing section: {section}...")

    if section == 'methods' and parsed_script['methods']:
        for method, method_code in parsed_script['methods'].items():
            # Mitigate Code
            new_code_block = run_mitigation_loop(
                bandit_allowance,
                base_name,
                llm,
                logger,
                mitigated_folder,
                mitigated_base_name,
                associated_letter_conversion,
                method_code,
                method)

            # Update code via input section
            new_parsed_script['methods'][method] = new_code_block

    elif section != 'main_script' and parsed_script[section]:
        new_code_block = run_mitigation_loop(
            bandit_allowance,
            base_name,
            llm,
            logger,
            mitigated_folder,
            mitigated_base_name,
            associated_letter_conversion,
            code,
            section)

        # Update code via input section
        new_parsed_script[section] = new_code_block

    else:
        logger.info(f"No code found in section {section}.")

# Define filenames and paths for consolidated and organized code files
consolidated_file_name = f"{mitigated_base_name}_consolidated{ext}"
consolidated_file_path = os.path.join(mitigated_folder, consolidated_file_name)

# Write the consolidated code sections into a single file
with open(consolidated_file_path, 'w') as file:
    # Write global variables section if it exists
    if new_parsed_script['global_variables']:
        file.write(new_parsed_script['global_variables'] + '\n\n')

    # Write methods in alphabetical order
    for method_name in sorted(new_parsed_script['methods']):
        file.write(new_parsed_script['methods'][method_name] + '\n\n')

    # Write main script section if it exists
    if new_parsed_script['main_script']:
        file.write(new_parsed_script['main_script'] + '\n')

# Organize imports and global variables in the consolidated code
organized_fixed_code = organize_imports_and_globals(consolidated_file_path)

# Define filename and path for the organized code file
organized_file_name = f"{mitigated_base_name}_organized{ext}"
organized_file_path = os.path.join(mitigated_folder, organized_file_name)

# Write the organized code to a file
write_to_file(organized_file_path, organized_fixed_code)

# Read the contents of the organized file
with open(organized_file_path, 'r') as file:
    code_content = file.read()

# Use autoimport to fix import issues
fixed_code = autoimport.fix_code(code_content)

# Specify the path to the GGUF model file
model_path = "models/llama-3.2-3b-instruct-q8_0.gguf"

# Retrieve the total number of layers in the model
total_layers = get_block_count_keys(model_path, logger)

# Determine the optimal number of layers to offload to the GPU
n_gpu_layers = get_optimal_gpu_layers(model_path, total_layers)

# Initialize the Llama model with specified settings
llm = Llama(
    model_path=model_path,  # Path to the GGUF model file
    seed=42,  # Fixed seed for reproducibility
    n_ctx=4096,  # Context size for the model
    use_mmap=True,  # Enable memory mapping for efficiency
    use_mlock=True,  # Prevent swapping to disk for consistent performance
    n_gpu_layers=n_gpu_layers  # Number of layers to offload to GPU
)

# Prepare the system prompt for the AI assistant
unit_test_system_prompt = (
    f"You are an AI programming assistant proficient in Python application development. You excel at identifying and"
    f" refactoring code to improve security, readability, and efficiency while adhering to best practices. Your "
    f"guidance must be concise, actionable, and prioritize abstraction when reviewing and refactoring the code."
)

# Prepare the user prompt with the code to be reviewed
unit_test_user_prompt = (
    f"Please review and refactor the following Python application code. Focus on abstracting redundant logic and "
    f"improving maintainability without changing the intended behavior or adding any new variables. "
    f"Use concise comments to explain critical modifications:```python\n{fixed_code}\n```"
)

# Combine system and user prompts into a message list
unit_test_messages = [
    {"role": "system", "content": unit_test_system_prompt},
    {"role": "user", "content": unit_test_user_prompt}
]

# Create chat completion llama object
consolidated_llm_config = create_chat_completion_llm(
    llm,
    unit_test_messages,
    0.0,
    1,
    0,
    True,
    ["<|endoftext|>"]
)

# Process the streamed output to obtain the adjusted code block
consolidated_code_response = process_streamed_output(consolidated_llm_config)

# Extract the code block from the AI's response
mitigated_code = extract_code_block(consolidated_code_response)

# Define filename and path for the final output file
final_file_name = f"{mitigated_base_name}_final{ext}"
final_mitigated_file_path = os.path.join(mitigated_folder, final_file_name)

# Save the refined code to the final output file
write_to_file(final_mitigated_file_path, mitigated_code)

# Extract the libraries from the mitigated code
extracted_libraries = extract_libraries(mitigated_code)

# Perform scans
continue_boolean = perform_scans(final_mitigated_file_path, associated_letter_conversion, extracted_libraries, logger)

if continue_boolean:
    logger.info(f"Generating unit test file to test {input_file_path} and {final_mitigated_file_path}.")

    # Define filename and path for the final output file
    unit_test_file_name = f"{mitigated_base_name}_unit_test{ext}"
    unit_test_file_path = os.path.join(generated_unit_test_dir, name, associated_letter_conversion, unit_test_file_name)

    # Create the directory for the specialized path for the generated unit test
    os.makedirs(os.path.dirname(unit_test_file_path))

    # # Prepare the system prompt for the AI assistant
    # unit_test_system_prompt = (
    #     "You are an AI assistant skilled in generating Python unit tests. Your task is to create a unit test that executes "
    #     "both the original and mitigated functions, providing the same input to each, capturing their outputs, and comparing "
    #     "them to ensure they produce identical results. If they do not, the test should identify the discrepancies. "
    #     "Additionally, the test should include example inputs and their expected outputs. "
    #     "Please ensure that all explanations and instructions are embedded within the code as docstrings or inline comments, "
    #     "adhering to Python's best practices for documentation."
    # )
    #
    # # Prepare the user prompt with the code to be reviewed
    # unit_test_user_prompt = (
    #     "Below are two Python functions:\n\n"
    #     "Original Function:\n"
    #     "```\n"
    #     f"\"\"{original_code}\"\"\"\n"
    #     "```\n\n"
    #     "Mitigated Function:\n"
    #     "```\n"
    #     f"\"\"{mitigated_code}\"\"\"\n"
    #     "```\n\n"
    #     "Please generate a Python unit test that:\n"
    #     "1. Defines both functions within the test script.\n"
    #     "2. Provides example inputs and their expected outputs.\n"
    #     "3. Executes both functions with the same inputs.\n"
    #     "4. Captures and compares their outputs.\n"
    #     "5. Asserts that the outputs are identical.\n"
    #     "6. Reports any discrepancies if the outputs differ.\n"
    #     "If creating such a unit test is not feasible due to differences in the functions or other limitations, please provide a detailed explanation. "
    #     "Ensure that all explanations and instructions are included as docstrings or inline comments within the code, following Python's best practices for documentation."
    # )

    # Combine system and user prompts into a message list

    unit_test_system_prompt = (
        "You are an AI assistant proficient in generating Python unit tests. Your responsibilities include:\n"
        "1. Analyzing provided Python script's methods so that ONLY the expected input types and formats are tested.\n"
        "2. Creating a minimal series of unit tests that validate the script's behavior with the exact inputs accepted by each script's methods.\n"
        "3. Embedding explanations and instructions within the code as docstrings or inline comments, following Python's best practices for documentation.\n\n"
        "# For each method being tested, ensure this import guideline `# Replace 'your_module' with the actual module name` is clearly included next to each method to test."
    )

    unit_test_user_prompt = (
        "Please generate a minimally viable comprehensive Python unit test for the mitigated code below: \n"
        "Mitigated Script:\n"
        "```\n"
        f"\"\"{mitigated_code}\"\"\"\n"
        "```\n\n"
    )

    unit_test_messages = [
        {"role": "system", "content": unit_test_system_prompt},
        {"role": "user", "content": unit_test_user_prompt}
    ]

    unit_test_llm_config = llm.create_chat_completion(
        messages=unit_test_messages,
        temperature=0.0,  # Set temperature for deterministic output
        top_p=1,  # Use nucleus sampling with top_p probability
        top_k=0,  # Disable top-k sampling
        stream=True,  # Enable streaming of the response
        stop=["<|endoftext|>"]  # Define stopping criteria for generation
    )

    # Process the streamed output to obtain the adjusted code block
    unit_test_response = process_streamed_output(unit_test_llm_config)

    # Extract the code block from the AI's response
    unit_test_code = extract_code_block(unit_test_response)

    # Save the refined code to the final output file
    write_to_file(unit_test_file_path, unit_test_code)

else:
    logger.error(f"Issues still exist with {final_mitigated_file_path}. See above file scan mentions.")
