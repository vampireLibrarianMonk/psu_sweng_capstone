import argparse
import json
import os
import re
from llama_cpp import Llama
import tiktoken

from utilities import setup_logger
from vulnerable_file_detector import analyze_file_with_bandit

# Set up argument parser
parser = argparse.ArgumentParser(description='Process a Python file for vulnerability assessment.')
parser.add_argument('file_path', help='Path to the Python file to be analyzed')

# Parse command-line arguments
args = parser.parse_args()

input_file_path = args.file_path

# Get and separate file name into its base name and extension
base_name = os.path.basename(input_file_path)
name, ext = os.path.splitext(base_name)

# Initialize the logger
logger = setup_logger(f"vulnerability_rectifier_{name}")

# Read the specified file
try:
    with open(input_file_path, 'r') as file:
        original_code = file.read()
        logger.info(f"Successfully read file: {input_file_path}")
except FileNotFoundError:
    logger.error(f"The file '{input_file_path}' was not found.")
    exit(1)
except IOError as e:
    logger.error(f"An error occurred while reading the file '{input_file_path}': {e}")
    exit(1)

def extract_code_block(text):
    """
    Extracts the first Python code block from the given text.

    Args:
        text (str): The input text containing code blocks.

    Returns:
        str: The extracted code block, or None if no code block is found.
    """
    match = re.search(r'```python\n(.*?)```', text, re.DOTALL)
    return match.group(1) if match else None

def extract_secure_implementation_title(text):
    pattern = r"\*\*Secure Implementation:(.*?)\*\*"
    match = re.search(pattern, text, re.DOTALL)
    return match.group(1).strip().replace("\n", "") if match else None

def extract_secure_implementation_statement(text):
    pattern = r"\*\*Secure Implementation Statement:?\*\*(.*?)\n+```python"
    match = re.search(pattern, text, re.DOTALL)
    return match.group(1).strip().replace("\n", "") if match else None

def extract_secure_implementation_explanation_statement(text):
    pattern = r"\*\*Secure Code Implementation Statement:\*\*(.*)\n+\*\*END\*\*"
    match = re.search(pattern, text, re.DOTALL)
    return match.group(1).strip().replace("\n", "") if match else None

def save_code_to_file(code, path):
    """
    Saves the given code to a specified file.

    Args:
        code (str): The code to be saved.
        path (str): The path to save the code in.
    """
    with open(path, 'w') as file_io:
        file_io.write(code)
    logger.info(f"Code saved to file: {path}")

mitigated_folder = "mitigated_files"

os.makedirs(mitigated_folder, exist_ok=True)
logger.info(f"Ensured mitigated folder exists: {mitigated_folder}")

# Specify the path to your GGUF model file
# model_path = "models/llama-3.2-3b-instruct-q8_0.gguf"
model_path = "bartowski/Llama-3-11.5B-Instruct-Coder-v2-GGUF"
logger.info(f"Model path set to: {model_path}")

# Initialize the Llama model with appropriate settings
llm = Llama(
    model_path=model_path,
    seed=42,  # Fixed seed for reproducibility
    use_mmap=True,  # Memory mapping for efficiency
    use_mlock=True  # Prevent swapping to disk for consistent performance
)

# Define the system and user prompts
system_prompt = (
    "A chat between a developer and an artificial intelligence programming assistant fluent in secure programming techniques. "
    "The assistant gives a single concise, secure, detailed and polite answers to the user's questions."
)

user_prompt = (
    "Provide a single, concise sentence suggesting the best way to fix this Python code. The suggestion should "
    "focus on replacing unsafe practices with secure alternatives without describing specific methods or "
    "implementations.\n\n"
    f"```python\n{original_code}\n```"
)

# Create the chat messages sequence
messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_prompt}
]

# Initialize the tokenizer for GGUF models
tokenizer = tiktoken.get_encoding("cl100k_base")

# Calculate the number of tokens in the user prompt
input_tokens = len(tokenizer.encode(user_prompt))
logger.debug(f"Input tokens: {input_tokens}")

# Dynamically calculate max_tokens (e.g., 4x input tokens, capped at 4096)
max_tokens = min(input_tokens * 4, 4096)

# Generate the assessment using chat completion
assessment_response = llm.create_chat_completion(
    messages=messages,
    max_tokens=max_tokens,
    temperature=0.0,
    top_p=1.0,
    top_k=0
)

# Extract and process the assessment
assessment = assessment_response['choices'][0]['message']['content'].strip()
logger.info(f"Vulnerability Assessment: {assessment}")

# Define the system and user prompts
secure_system_prompt = (
    "A chat between a developer and an artificial intelligence programming assistant fluent in secure programming techniques. "
    "The assistant gives concise, secure, detailed and pythonic answers to the user's questions."
    "The structure of the response will be **Secure Implementation Title**, then the **Secure Implementation Statement**, then the secure code implementation code within ```python```, then **Secure Code Implementation Statement** and then finally **END**"
)


# Generate secure code suggestion
secure_code_prompt = (
    f"Based on the identified vulnerability assessment {assessment}, provide a secure version of the following Python code:\n\n"
    f"```python\n{original_code}\n```"
)

messages = [
    {"role": "system", "content": secure_system_prompt},
    {"role": "user", "content": secure_code_prompt}
]

input_tokens = len(tokenizer.encode(secure_code_prompt))
max_tokens = min(input_tokens * 8, 2048)

secure_code_response = llm.create_chat_completion(
    messages=messages,
    max_tokens=max_tokens,
    temperature=0.0,
    top_p=1.0,
    top_k=0,
    stop=["\n**Explanation:**\n"]
)

secure_code = secure_code_response['choices'][0]['message']['content'].strip()
secure_title = extract_secure_implementation_title(secure_code)
secure_implementation_statement = extract_secure_implementation_statement(secure_code)
secure_implementation_explanation_statement = extract_secure_implementation_explanation_statement(secure_code)
logger.info(f"Secure Implementation Title: {secure_title}")
logger.info(f"Secure Implementation Statement: {secure_implementation_statement}")
logger.info(f"Secure Implementation Explanation Statement: {secure_implementation_explanation_statement}")

# Extract the code block from the secure code suggestion
code_block = extract_code_block(secure_code)

if code_block:
    name, ext = os.path.splitext(base_name)
    mitigated_file_name = f"{name}_mitigated{ext}"
    mitigated_file_path = os.path.join(mitigated_folder, mitigated_file_name)
    save_code_to_file(code_block, mitigated_file_path)

    iteration = 1

    bandit_scan_json_path = analyze_file_with_bandit(mitigated_file_path)

    # Open and read the JSON file
    with open(bandit_scan_json_path, 'r') as file:
        data = json.load(file)

    # Map test_id to (line_number, line_range, issue_text)
    test_id_mapping_with_range = {
        result["test_id"]: (result["line_number"], result["line_range"], result["issue_text"])
        for result in data["results"]
    }

    # Read the specified file
    try:
        with open(mitigated_file_path, 'r') as file:
            mitigated_code = file.read()
            logger.info(f"Successfully read file: {mitigated_file_path}")
    except FileNotFoundError:
        logger.error(f"The file '{mitigated_file_path}' was not found.")
        exit(1)
    except IOError as e:
        logger.error(f"An error occurred while reading the file '{mitigated_file_path}': {e}")
        exit(1)

    # Construct Bandit issues for user prompt
    bandit_issues = "\n".join(
        f"- Test ID: {test_id}, Line: {details[0]}, Line Range: {details[1]}, Issue: {details[2]}"
        for test_id, details in test_id_mapping_with_range.items()
    )

    # Define the system and user prompts
    bandit_system_prompt = (
        "A chat between a developer and an artificial intelligence programming assistant fluent in secure programming techniques. "
        "The assistant uses Bandit's findings to identify and resolve specific vulnerabilities in the provided code. "
        "The response will include **Secure Implementation Title**, **Secure Implementation Statement**, "
        "the updated secure code implementation within ```python``` blocks, a **Secure Code Implementation Statement**, "
        "and finally **END**."
    )

    bandit_code_prompt = (
        f"The Bandit static analysis tool has identified the following issues in the provided Python code:\n"
        f"{bandit_issues}\n\n"
        f"Please analyze and provide a secure version of the code below, addressing these specific issues:\n\n"
        f"```python\n{mitigated_code}\n```"
    )

    messages = [
        {"role": "system", "content": bandit_system_prompt},
        {"role": "user", "content": bandit_code_prompt}
    ]

    input_tokens = len(tokenizer.encode(bandit_code_prompt))
    max_tokens = min(input_tokens * 16, 4096)


    # Streaming function to handle the output
    def process_streamed_output(streamed_response):
        """
        Processes the streamed output chunk by chunk.

        Args:
            streamed_response (iterator): An iterator yielding chunks of response content.

        Returns:
            str: The complete output from the stream.
        """
        complete_output = ""
        for chunk in streamed_response:
            content = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
            print(content, end="", flush=True)  # Print in real-time
            complete_output += content
        print("\n")  # Ensure a new line after the streamed output
        return complete_output


    # Create a chat completion request with streaming enabled
    logger.info("Starting secure code generation with streaming.")
    streamed_response = llm.create_chat_completion(
        messages=messages,
        max_tokens=max_tokens,
        temperature=0.0,
        top_p=1.0,
        top_k=0,
        stream=True,  # Enable streaming
        stop=["\n**END**"]
    )

    # Process the streamed response
    adjusted_secure_code = process_streamed_output(streamed_response)
    logger.info("Secure code generation completed.")

    # Extract additional components from the streamed response
    adjusted_secure_title = extract_secure_implementation_title(adjusted_secure_code)
    adjusted_secure_implementation_statement = extract_secure_implementation_statement(adjusted_secure_code)
    adjusted_secure_implementation_explanation_statement = extract_secure_implementation_explanation_statement(
        adjusted_secure_code)
    logger.info(f"Adjusted Secure Implementation Title: {adjusted_secure_title}")
    logger.info(f"Adjusted Secure Implementation Statement: {adjusted_secure_implementation_statement}")
    logger.info(
        f"Adjusted Secure Implementation Explanation Statement: {adjusted_secure_implementation_explanation_statement}")

    # Extract the code block from the secure code suggestion
    code_block = extract_code_block(adjusted_secure_code)

    if code_block:
        name, ext = os.path.splitext(base_name)
        mitigated_file_name = f"{name}_mitigated_iteration{iteration}_{ext}"
        mitigated_file_path = os.path.join(mitigated_folder, mitigated_file_name)
        save_code_to_file(code_block, mitigated_file_path)
    else:
        logger.warning("No adjusted code block found in the secure code suggestion.")

else:
    logger.warning("No code block found in the secure code suggestion.")

