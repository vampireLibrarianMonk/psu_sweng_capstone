import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

def generate_secure_code():
    model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct"

    try:
        # Load tokenizer and model
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)

        # Initialize the text generation pipeline
        code_generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

        # Define the prompt with the vulnerable code
        prompt = (
            "The following Python code contains a vulnerability due to the use of `pickle` for deserialization, "
            "which is unsafe for untrusted input. Rewrite the code to use a safe alternative like `json` and "
            "provide only the corrected code:\n\n"
            "```python\n"
            "import pickle\n\n"
            "def load_user_data(serialized_data):\n"
            "    user_data = pickle.loads(serialized_data)  # Untrusted input deserialization\n"
            "    print(f\"User data: {user_data}\")\n"
            "```"
        )

        # Generate the secure code suggestion
        print("Generating secure code suggestion...")
        suggestions = code_generator(prompt, max_length=150, num_return_sequences=1)

        # Extract and print the generated code
        secure_code = suggestions[0]["generated_text"].strip()
        print("Suggested Secure Code:\n")
        print(secure_code)

    except Exception as e:
        print(f"An error occurred: {e}")

if __name__ == "__main__":
    generate_secure_code()