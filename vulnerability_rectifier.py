import argparse
import os
import re
from llama_cpp import Llama
import tiktoken

from utilities import setup_logger

# Set up argument parser
parser = argparse.ArgumentParser(description='Process a Python file for vulnerability assessment.')
parser.add_argument('file_path', help='Path to the Python file to be analyzed')

# Parse command-line arguments
args = parser.parse_args()

input_file_path = args.file_path

# Get and separate file name into its base name and extension
base_name = os.path.basename(input_file_path)
name, ext = os.path.splitext(base_name)

# Initialize the logger
logger = setup_logger(name)

# Read the specified file
try:
    with open(input_file_path, 'r') as file:
        original_code = file.read()
        logger.info(f"Successfully read file: {input_file_path}")
except FileNotFoundError:
    logger.error(f"The file '{input_file_path}' was not found.")
    exit(1)
except IOError as e:
    logger.error(f"An error occurred while reading the file '{input_file_path}': {e}")
    exit(1)

def extract_code_block(text):
    """
    Extracts the first Python code block from the given text.

    Args:
        text (str): The input text containing code blocks.

    Returns:
        str: The extracted code block, or None if no code block is found.
    """
    match = re.search(r'```python\n(.*?)```', text, re.DOTALL)
    return match.group(1) if match else None

def extract_secure_implementation_title(text):
    pattern = r"\*\*Secure Implementation:(.*?)\*\*"
    match = re.search(pattern, text, re.DOTALL)
    return match.group(1).strip().replace("\n", "") if match else None

def extract_secure_implementation_statement(text):
    pattern = r"\*\*Secure Implementation Statement:?\*\*(.*?)\n+```python"
    match = re.search(pattern, text, re.DOTALL)
    return match.group(1).strip().replace("\n", "") if match else None

def extract_secure_implementation_explanation_statement(text):
    pattern = r"\*\*Secure Code Implementation Statement:\*\*(.*)\n+\*\*END\*\*"
    match = re.search(pattern, text, re.DOTALL)
    return match.group(1).strip().replace("\n", "") if match else None

def save_code_to_file(code, path):
    """
    Saves the given code to a specified file.

    Args:
        code (str): The code to be saved.
        path (str): The path to save the code in.
    """
    with open(path, 'w') as file_io:
        file_io.write(code)
    logger.info(f"Code saved to file: {path}")

mitigated_folder = "mitigated_files"

os.makedirs(mitigated_folder, exist_ok=True)
logger.info(f"Ensured mitigated folder exists: {mitigated_folder}")

# Specify the path to your GGUF model file
model_path = "models/llama-3.2-3b-instruct-q8_0.gguf"
logger.info(f"Model path set to: {model_path}")

# Initialize the Llama model with appropriate settings
llm = Llama(
    model_path=model_path,
    seed=42,  # Fixed seed for reproducibility
    use_mmap=True,  # Memory mapping for efficiency
    use_mlock=True  # Prevent swapping to disk for consistent performance
)

# Define the system and user prompts
system_prompt = (
    "A chat between a developer and an artificial intelligence programming assistant fluent in secure programming techniques. "
    "The assistant gives a single concise, secure, detailed and polite answers to the user's questions."
)

user_prompt = (
    "Provide a single, concise sentence suggesting the best way to fix this Python code. The suggestion should "
    "focus on replacing unsafe practices with secure alternatives without describing specific methods or "
    "implementations.\n\n"
    f"```python\n{original_code}\n```"
)

# Create the chat messages sequence
messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_prompt}
]

# Initialize the tokenizer for GGUF models
tokenizer = tiktoken.get_encoding("cl100k_base")

# Calculate the number of tokens in the user prompt
input_tokens = len(tokenizer.encode(user_prompt))
logger.debug(f"Input tokens: {input_tokens}")

# Dynamically calculate max_tokens (e.g., 4x input tokens, capped at 4096)
max_tokens = min(input_tokens * 4, 4096)

# Generate the assessment using chat completion
assessment_response = llm.create_chat_completion(
    messages=messages,
    max_tokens=max_tokens,
    temperature=0.0,
    top_p=1.0,
    top_k=0
)

# Extract and process the assessment
assessment = assessment_response['choices'][0]['message']['content'].strip()
logger.info(f"Vulnerability Assessment: {assessment}")

# Define the system and user prompts
secure_system_prompt = (
    "A chat between a developer and an artificial intelligence programming assistant fluent in secure programming techniques. "
    "The assistant gives concise, secure, detailed and pythonic answers to the user's questions."
    "The structure of the response will be **Secure Implementation Title**, then the **Secure Implementation Statement**, then the secure code implementation code within ```python```, then **Secure Code Implementation Statement** and then finally **END**"
)


# Generate secure code suggestion
secure_code_prompt = (
    f"Based on the identified vulnerability assessment {assessment}, provide a secure version of the following Python code:\n\n"
    f"```python\n{original_code}\n```"
)

messages = [
    {"role": "system", "content": secure_system_prompt},
    {"role": "user", "content": secure_code_prompt}
]

input_tokens = len(tokenizer.encode(secure_code_prompt))
max_tokens = min(input_tokens * 8, 2048)

secure_code_response = llm.create_chat_completion(
    messages=messages,
    max_tokens=max_tokens,
    temperature=0.0,
    top_p=1.0,
    top_k=0,
    stop=["\n**Explanation:**\n"]
)

secure_code = secure_code_response['choices'][0]['message']['content'].strip()
print(secure_code)
secure_title = extract_secure_implementation_title(secure_code)
secure_implementation_statement = extract_secure_implementation_statement(secure_code)
secure_implementation_explanation_statement = extract_secure_implementation_explanation_statement(secure_code)
logger.info(f"Secure Implementation Title: {secure_title}")
logger.info(f"Secure Implementation Statement: {secure_implementation_statement}")
logger.info(f"Secure Implementation Explanation Statement: {secure_implementation_explanation_statement}")

# Extract the code block from the secure code suggestion
code_block = extract_code_block(secure_code)

if code_block:
    name, ext = os.path.splitext(base_name)
    mitigated_file_name = f"{name}_mitigated{ext}"
    mitigated_file_path = os.path.join(mitigated_folder, mitigated_file_name)
    save_code_to_file(code_block, mitigated_file_path)
else:
    logger.warning("No code block found in the secure code suggestion.")
