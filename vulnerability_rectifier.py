import argparse
import os
import sys

import autoimport
from llama_cpp import Llama

from utilities_llm import (setup_logger,
                           get_optimal_gpu_layers,
                           get_block_count_keys,
                           validate_bandit_allowance,
                           parse_python_script, run_mitigation_loop,
                           organize_imports_and_globals, write_to_file, process_streamed_output, extract_code_block,
                           extract_libraries, perform_scans, is_python_file, count_directories_in_path,
                           number_to_excel_column, create_chat_completion_llm, generate_import_statement,
                           get_methods_from_file, get_methods_with_signatures, prepend_to_file, execute_unit_test_file,
                           rename_file
                           )

mitigation_folder = "mitigated_files"

os.makedirs(mitigation_folder, exist_ok=True)

generated_unit_test_dir = "generated_unit_test"

os.makedirs(generated_unit_test_dir, exist_ok=True)

# Specify the path to your GGUF model file
model_path = "models/llama-3.2-3b-instruct-q8_0.gguf"

# Set up argument parser
parser = argparse.ArgumentParser(description='Process a Python file for vulnerability assessment.')
parser.add_argument('file_path', help='Path to the Python file to be analyzed.')
parser.add_argument('bandit_allowance', type=validate_bandit_allowance, help='Allowed number of Bandit '
                                                                             'mitigation runs (must be a positive '
                                                                             'integer).')

# Parse command-line arguments
args = parser.parse_args()

input_file_path = args.file_path
bandit_allowance = args.bandit_allowance

# Vet the input file as a python script file
if is_python_file(input_file_path):
    with open(input_file_path, 'r') as file:
        original_code = file.read()
else:
    print(f"{input_file_path} is not a valid Python file.")
    sys.exit(1)

# Get and separate file name into its base name and extension
base_name = os.path.basename(input_file_path)
name, ext = os.path.splitext(base_name)

# Read the specified file and use ast to parse it into a map for quicker analysis and mitigation
parsed_script = parse_python_script(input_file_path)

new_parsed_script = {
    'global_variables': '',
    'methods': {},
    'main_script': ''
}

# Get method count of parsed script
method_count = len(parsed_script['methods'].items())

# Define base name for mitigated files
mitigated_base_name = f"{name}_mitigated"

# Get the file count
file_count = count_directories_in_path(os.path.join(mitigation_folder, mitigated_base_name))

# Create folder for mitigated files
associated_letter_conversion = number_to_excel_column(file_count)
mitigated_folder = os.path.join(mitigation_folder, mitigated_base_name, associated_letter_conversion)

# Initialize the logger
logger = setup_logger(f"vulnerability_rectifier", associated_letter_conversion, name)

os.makedirs(mitigated_folder, exist_ok=True)

# Alternative way for getting block count using Llama
# Load the model
# llm = Llama(model_path=model_path)

# Access the model's metadata
# metadata = llm.metadata

# Retrieve the block count
# total_layers = int(metadata.get('llama.block_count'))

# Get the total layers from the input model
total_layers = get_block_count_keys(model_path, logger)

# Determine the optimal number of GPU layers to offload
n_gpu_layers = get_optimal_gpu_layers(model_path, total_layers)

# Initialize the Llama model with appropriate settings
llm = Llama(
    model_path=model_path,
    seed=42,  # Fixed seed for reproducibility
    n_ctx=4096,  # Set the desired context size here
    use_mmap=True,  # Memory mapping for efficiency
    use_mlock=True,  # Prevent swapping to disk for consistent performance
    n_gpu_layers=n_gpu_layers
)

for section, code in parsed_script.items():
    logger.info(f"Processing section: {section}...")

    if section == 'methods' and parsed_script['methods']:
        for method, method_code in parsed_script['methods'].items():
            # Mitigate Code
            new_code_block = run_mitigation_loop(
                bandit_allowance,
                base_name,
                llm,
                logger,
                mitigated_folder,
                mitigated_base_name,
                associated_letter_conversion,
                method_code,
                method)

            # Update code via input section
            new_parsed_script['methods'][method] = new_code_block

    elif section != 'main_script' and parsed_script[section]:
        new_code_block = run_mitigation_loop(
            bandit_allowance,
            base_name,
            llm,
            logger,
            mitigated_folder,
            mitigated_base_name,
            associated_letter_conversion,
            code,
            section)

        # Update code via input section
        new_parsed_script[section] = new_code_block

    else:
        logger.info(f"No code found in section {section}.")

# Define filenames and paths for consolidated and organized code files
consolidated_file_name = f"{mitigated_base_name}_consolidated{ext}"
consolidated_file_path = os.path.join(mitigated_folder, consolidated_file_name)

# Write the consolidated code sections into a single file
with open(consolidated_file_path, 'w') as file:
    # Write global variables section if it exists
    if new_parsed_script['global_variables']:
        file.write(new_parsed_script['global_variables'] + '\n\n')

    # Write methods in alphabetical order
    for method_name in sorted(new_parsed_script['methods']):
        file.write(new_parsed_script['methods'][method_name] + '\n\n')

    # Write main script section if it exists
    if new_parsed_script['main_script']:
        file.write(new_parsed_script['main_script'] + '\n')

# Organize imports and global variables in the consolidated code
organized_fixed_code = organize_imports_and_globals(consolidated_file_path)

# Define filename and path for the organized code file
organized_file_name = f"{mitigated_base_name}_organized{ext}"
organized_file_path = os.path.join(mitigated_folder, organized_file_name)

# Write the organized code to a file
write_to_file(organized_file_path, organized_fixed_code)

# Read the contents of the organized file
with open(organized_file_path, 'r') as file:
    code_content = file.read()

# Use autoimport to fix import issues
fixed_code = autoimport.fix_code(code_content)

# Specify the path to the GGUF model file
model_path = "models/llama-3.2-3b-instruct-q8_0.gguf"

# Retrieve the total number of layers in the model
total_layers = get_block_count_keys(model_path, logger)

# Determine the optimal number of layers to offload to the GPU
n_gpu_layers = get_optimal_gpu_layers(model_path, total_layers)

# Initialize the Llama model with specified settings
llm = Llama(
    model_path=model_path,  # Path to the GGUF model file
    seed=42,  # Fixed seed for reproducibility
    n_ctx=4096,  # Context size for the model
    use_mmap=True,  # Enable memory mapping for efficiency
    use_mlock=True,  # Prevent swapping to disk for consistent performance
    n_gpu_layers=n_gpu_layers  # Number of layers to offload to GPU
)

# Prepare the system prompt for the AI assistant
final_script_system_prompt = (
    "You are an AI programming assistant proficient in Python application development. Your expertise includes "
    "identifying and refactoring code to enhance security, readability, and efficiency, while adhering to best "
    "practices. Your guidance should be concise, actionable, and prioritize abstraction when reviewing and "
    "refactoring code."
)

# Prepare the user prompt with the code to be reviewed
final_script_user_prompt = (
    f"Please review and refactor the following Python code. Focus on abstracting redundant logic, improving "
    f"maintainability, and implementing structured error handling to manage unexpected input types. Ensure exceptions "
    f"are handled by first addressing those specific to the method's functionality, followed by those from associated "
    f"libraries, and finally more general exceptions, all while preserving their original types and messages. Avoid "
    f"altering behavior or introducing additional variables. Use concise comments to explain critical modifications:\n"
    f"```python\n{fixed_code}\n```"
)

# Combine system and user prompts into a message list
final_script_messages = [
    {"role": "system", "content": final_script_system_prompt},
    {"role": "user", "content": final_script_user_prompt}
]

# Create chat completion llama object
final_llm_config = create_chat_completion_llm(
    llm,
    final_script_messages,
    0.0,
    1,
    0,
    True,
    ["<|endoftext|>"]
)

# Process the streamed output to obtain the adjusted code block
final_code_response = process_streamed_output(final_llm_config)

# Extract the code block from the AI's response
mitigated_code = extract_code_block(final_code_response)

# Define filename and path for the final output file
final_file_name = f"{mitigated_base_name}_final{ext}"
final_mitigated_file_path = os.path.join(mitigated_folder, final_file_name)

# Save the refined code to the final output file
write_to_file(final_mitigated_file_path, mitigated_code)

# Extract the libraries from the mitigated code
extracted_libraries = extract_libraries(mitigated_code)

# Perform scans
continue_boolean = perform_scans(final_mitigated_file_path, associated_letter_conversion, extracted_libraries, logger)

if continue_boolean:
    logger.info(f"Generating unit test summary for inclusion into a unit test file.")

    unit_test_summary_system_prompt = (
        "You are an AI assistant skilled in analyzing Python scripts to extract key information necessary for "
        "generating unit tests. Your task is to create a concise and structured summary of the provided script's "
        "methods, focusing on its functionality and expected inputs/outputs and error/exception handling. Ensure the "
        "summary is clear, organized, and useful for writing effective unit tests."
    )

    unit_test_summary_user_prompt = (
        "Summarize the Python script to generate one functional test, one edge case test and one exception test. "
        "Include:\n"
        "1. Key method with signature and purpose.\n"
        "2. Expected input and output for each method.\n"
        "3. One potential edge case.\n\n"
        "Script:\n"
        "```\n"
        f"{mitigated_code}\n"
        "```"
    )

    unit_test_messages = [
        {"role": "system", "content": unit_test_summary_system_prompt},
        {"role": "user", "content": unit_test_summary_user_prompt}
    ]

    unit_test_summary_llm_config = llm.create_chat_completion(
        messages=unit_test_messages,
        temperature=0.0,  # Set temperature for deterministic output
        top_p=1,  # Use nucleus sampling with top_p probability
        top_k=0,  # Disable top-k sampling
        stream=True,  # Enable streaming of the response
        stop=["<|endoftext|>"]  # Define stopping criteria for generation
    )

    # Process the streamed output to obtain the adjusted code block
    unit_test_summary_response = process_streamed_output(unit_test_summary_llm_config)

    logger.info(f"Generating unit test file to test {input_file_path} and {final_mitigated_file_path}.")

    # Get methods from final mitigated file and generate import statements for the programmatically generated unit test
    method_list = get_methods_from_file(final_mitigated_file_path)
    import_list = [f"{generate_import_statement(final_mitigated_file_path)} import {method}" for method in method_list]
    import_string = '\n'.join(import_list)
    # signatures_list = get_methods_with_signatures(final_mitigated_file_path)
    # signatures_string = '\n'.join(f"{name}: {signature} -> {rtype}" for name, signature, rtype in signatures_list)

    unit_test_system_prompt = (
        "You are an AI assistant skilled in generating robust and effective Python unit tests using the pytest "
        "framework. Your goal is to ensure code correctness, comprehensive edge case coverage, and effective validation"
        " of behavior. Use only pytest and do not include any additional from/import statements."
    )

    unit_test_user_prompt = (
        "Based exclusively on the provided Mitigated Script Analysis, generate the prescribed Python unit tests:\n\n"
        "Mitigated Script Analysis:\n"
        f"{unit_test_summary_response}\n"
    )

    unit_test_messages = [
        {"role": "system", "content": unit_test_system_prompt},
        {"role": "user", "content": unit_test_user_prompt}
    ]

    unit_test_llm_config = llm.create_chat_completion(
        messages=unit_test_messages,
        temperature=0.0,  # Set temperature for deterministic output
        top_p=1,  # Use nucleus sampling with top_p probability
        top_k=0,  # Disable top-k sampling
        stream=True,  # Enable streaming of the response
        stop=["<|endoftext|>"]  # Define stopping criteria for generation
    )

    # Process the streamed output to obtain the adjusted code block
    unit_test_response = process_streamed_output(unit_test_llm_config)

    # Extract the code block from the AI's response
    unit_test_code = extract_code_block(unit_test_response)

    # Use autoimport to rectify libraries entries
    fixed_unit_test_code = autoimport.fix_code(unit_test_code)

    # Remove the import example line artifact
    fixed_unit_test_code = fixed_unit_test_code.replace("from your_module import load_user_data", "")

    # Loop to correct errors iteratively
    iteration = 0
    correction_limit = bandit_allowance

    # Define filename and path for the final output file
    unit_test_file_name = f"{mitigated_base_name}_unit_test_{iteration}{ext}"
    unit_test_file_path = os.path.join(generated_unit_test_dir, name, associated_letter_conversion, unit_test_file_name)

    # Create the directory for the specialized path for the generated unit test
    os.makedirs(os.path.dirname(unit_test_file_path))

    # Save the refined code to the final output file
    write_to_file(unit_test_file_path, fixed_unit_test_code)

    # Prepend the generated import statements
    prepend_to_file(unit_test_file_path, import_string + "\n")

    while iteration < correction_limit:
        iteration += 1
        logger.info(f"Testing file {unit_test_file_path}.")
        stdout, stderr = execute_unit_test_file(unit_test_file_path)

        # Check if there are any errors in the test execution
        if "FAILED" not in stdout and "ERROR" not in stderr:
            # Define filename and path for the final output file
            final_unit_test_file_name = f"{mitigated_base_name}_unit_test_final{ext}"
            final_unit_test_file_path = os.path.join(generated_unit_test_dir, name, associated_letter_conversion,
                                               unit_test_file_name)
            rename_file(unit_test_file_path, final_unit_test_file_path)
            logger.info(f"All tests passed successfully for {final_unit_test_file_path}.")
            break

        logger.warning("Errors detected in the unit tests. Attempting to correct...")

        # Combine stdout and stderr for error analysis
        error_output = f"Standard Output:\n{stdout}\n\nStandard Error Output:\n{stderr}."

        # Prepare the LLM prompt to fix the errors
        error_fix_system_prompt = (
            "You are an AI assistant skilled in debugging and fixing Python unit tests written with the pytest "
            "framework. Analyze the following unit test code and the errors encountered during execution. Modify the "
            "code to fix only the errors while ensuring it maintains its original functionality."
        )

        # Read the contents of the organized file
        with open(unit_test_file_path, 'r') as file:
            unit_test_code_content = file.read()

        error_fix_user_prompt = (
            "Provide only the corrected unit test Python code from the below UNit Test Code and Errors Encountered:\n"
            f"Unit Test Code:\n"
            f"```{unit_test_code_content}\n```"
            f"Errors Encountered:\n"
            f"```{error_output}\n```"
        )

        error_fix_messages = [
            {"role": "system", "content": error_fix_system_prompt},
            {"role": "user", "content": error_fix_user_prompt}
        ]

        # Use the LLM to fix the errors
        error_fix_llm_config = llm.create_chat_completion(
            messages=error_fix_messages,
            temperature=0.0,  # Deterministic output
            top_p=1,         # Use nucleus sampling
            top_k=0,         # Disable top-k sampling
            stream=True,     # Enable streaming of the response
            stop=["<|endoftext|>"]  # Define stopping criteria
        )

        # Process the streamed output to obtain the corrected code
        corrected_unit_test_response = process_streamed_output(error_fix_llm_config)

        # Extract the corrected code block from the LLM response
        corrected_unit_test_code = extract_code_block(corrected_unit_test_response)

        # Use autoimport to fix any lingering import statements
        corrected_unit_test_code = autoimport.fix_code(corrected_unit_test_code)

        # Define filename and path for the final output file
        unit_test_file_name = f"{mitigated_base_name}_unit_test_{iteration}{ext}"
        unit_test_file_path = os.path.join(generated_unit_test_dir, name, associated_letter_conversion,
                                           unit_test_file_name)

        # Save the corrected code back to the unit test file
        write_to_file(unit_test_file_path, corrected_unit_test_code)

        # Prepend the generated import statements again if necessary
        prepend_to_file(unit_test_file_path, import_string + "\n")

else:
    logger.error(f"Issues still exist with {final_mitigated_file_path}. See above file scan mentions.")
