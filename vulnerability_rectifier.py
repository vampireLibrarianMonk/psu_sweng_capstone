from llama_cpp import Llama
import tiktoken

# Specify the path to your GGUF model file
model_path = "models/llama-3.2-3b-instruct-q8_0.gguf"

# Initialize the Llama model with appropriate settings
llm = Llama(
    model_path=model_path,
    seed=42,              # Fixed seed for reproducibility
    use_mmap=True,        # Memory mapping for efficiency
    use_mlock=True        # Prevent swapping to disk for consistent performance
)

# Load the vulnerable Python code from a file
file_path = "vulnerable_files/vulnerable.py"
with open(file_path, "r") as file:
    original_code = file.read()

# Define the system and user prompts
system_prompt = (
    "A chat between a developer and an artificial intelligence programming assistant fluent in secure programming techniques. "
    "The assistant gives concise, secure, detailed and polite answers to the user's questions."
)

user_prompt = (
            "Provide a single, concise sentence suggesting the best way to fix this Python code. The suggestion should "
            "focus on replacing unsafe practices with secure alternatives without describing specific methods or "
            "implementations.\n\n"
            f"```python\n{original_code}\n```"
        )

# Create the chat messages sequence
messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_prompt}
]

# Initialize the tokenizer for GGUF models
# Assuming "cl100k_base" works for your model; adjust if needed.
tokenizer = tiktoken.get_encoding("cl100k_base")

# Calculate the number of tokens in the user prompt
input_tokens = len(tokenizer.encode(user_prompt))

# Dynamically calculate max_tokens (e.g., 2x input tokens, capped at 4096)
max_tokens = min(input_tokens * 4, 4096)

# Generate the assessment using chat completion
assessment_response = llm.create_chat_completion(
    messages=messages,
    max_tokens=max_tokens,  # Dynamically calculated
    temperature=0.0,        # Deterministic output
    top_p=1.0,              # Consider all tokens
    top_k=0                 # Disable top-k sampling
)

# Extract and process the assessment
assessment = assessment_response['choices'][0]['message']['content'].strip()

# Print the assessment
print("Vulnerability Assessment:\n")
print(assessment)

# Prompt for secure code generation
secure_code_prompt = (
    f"Based on the identified vulnerability assessment {assessment}, provide a secure version of the following Python code:\n\n"
    f"```python\n{original_code}\n```"
)

# Generate secure code suggestion
secure_code_response = llm(secure_code_prompt, max_tokens=300, temperature=0.7)
secure_code = secure_code_response['choices'][0]['text'].strip()

# Print the secure code suggestion
print("\nSecure Code Suggestion:\n")
print(secure_code)
