import argparse
import autoimport
import json
import os

from llama_cpp import Llama

from utilities_llm import analyze_file_with_bandit, setup_logger, get_optimal_gpu_layers, get_block_count_keys, \
    validate_bandit_allowance, extract_libraries, extract_code_block, save_code_to_file, get_bandit_issues, \
    process_streamed_output, analyze_file_with_dodgy, get_dodgy_issues, analyze_file_with_semgrep, get_semgrep_issues

mitigated_folder = "mitigated_files"

os.makedirs(mitigated_folder, exist_ok=True)

# Specify the path to your GGUF model file
model_path = "models/llama-3.2-3b-instruct-q8_0.gguf"

# Set up argument parser
parser = argparse.ArgumentParser(description='Process a Python file for vulnerability assessment.')
parser.add_argument('file_path', help='Path to the Python file to be analyzed.')
parser.add_argument('bandit_allowance', help='Allowed iteration of bandit mitigation runs (inclusive).')

# Parse command-line arguments
args = parser.parse_args()

# Set up argument parser
parser = argparse.ArgumentParser(description='Process a Python file for vulnerability assessment.')
parser.add_argument('file_path', help='Path to the Python file to be analyzed.')
parser.add_argument('bandit_allowance', type=validate_bandit_allowance, help='Allowed number of Bandit '
                                                                             'mitigation runs (must be a positive '
                                                                             'integer).')

# Parse command-line arguments
args = parser.parse_args()

input_file_path = args.file_path
bandit_allowance = args.bandit_allowance

# Get and separate file name into its base name and extension
base_name = os.path.basename(input_file_path)
name, ext = os.path.splitext(base_name)

# Initialize the logger
logger = setup_logger(f"vulnerability_rectifier_{name}")

# Read the specified file
try:
    with open(input_file_path, 'r') as file:
        original_code = file.read()
        logger.info(f"Successfully read file: {input_file_path}")
except FileNotFoundError:
    logger.error(f"The file '{input_file_path}' was not found.")
    exit(1)
except IOError as e:
    logger.error(f"An error occurred while reading the file '{input_file_path}': {e}")
    exit(1)

# Get the total layers from the input model
total_layers = get_block_count_keys(model_path, logger)

# Determine the optimal number of GPU layers to offload
n_gpu_layers = get_optimal_gpu_layers(model_path, total_layers)

# Initialize the Llama model with appropriate settings
llm = Llama(
    model_path=model_path,
    seed=42,  # Fixed seed for reproducibility
    n_ctx=4096,  # Set the desired context size here
    use_mmap=True,  # Memory mapping for efficiency
    use_mlock=True,  # Prevent swapping to disk for consistent performance
    n_gpu_layers=n_gpu_layers
)

if original_code:
    # Set initial and maximum temperature values
    initial_temperature = 0.0
    max_temperature = 2.0

    # Calculate increment per iteration
    temperature_increment = (max_temperature - initial_temperature) / bandit_allowance

    # Define initial constraints based on the current mitigated code
    initial_line_count = len(original_code.split("\n"))
    initial_word_count = len(original_code.split())

    # Load the module substitution JSON file
    with open('substitution/module.json', 'r') as file:
        module_associations = json.load(file)
        if not isinstance(module_associations, dict):
            raise ValueError("The loaded substitutions JSON data is not a dictionary.")

    # Start with a blank suggestion map
    suggestion_map = {}

    iteration = 0
    name, ext = os.path.splitext(base_name)
    mitigated_file_name = f"{name}_mitigated_iteration_{iteration}{ext}"
    mitigated_file_path = os.path.join(mitigated_folder, mitigated_file_name)
    save_code_to_file(original_code, mitigated_file_path, logger)

    # Perform Vulnerability Scans
    # Static Code Analysis
    # Bandit is a static analysis tool designed to deeply inspect Python code for security issues.
    # It focuses on detecting vulnerabilities at the code level, such as the use of unsafe functions
    # (e.g., eval, exec), insecure cryptographic practices and code injection risks.
    bandit_scan_json_path = analyze_file_with_bandit(mitigated_file_path, logger)
    bandit_issues = get_bandit_issues(bandit_scan_json_path)

    # Dodgy is a lightweight security tool that complements Bandit by focusing on file-level issues.
    # It scans for hardcoded secrets (e.g., API keys, passwords), suspicious filenames (e.g., *.pem, id_rsa),
    # and insecure file paths (e.g., /tmp directories).
    dodgy_scan_json_path = analyze_file_with_dodgy(mitigated_file_path, logger)
    dodgy_issues = get_dodgy_issues(dodgy_scan_json_path)

    # Semgrep
    # Get libraries so the rulesets below (django and flask) can use them if they exist
    extracted_libraries = extract_libraries(original_code)
    # A lightweight static analysis tool that scans code for security vulnerabilities and enforces coding standards.
    # It offers customizable rules and supports taint analysis to track untrusted data through your codebase.
    semgrep_scan_json_path = analyze_file_with_semgrep(mitigated_file_path, extracted_libraries, logger)
    semgrep_issues = get_semgrep_issues(semgrep_scan_json_path)

    # Dependencies Scans
    # Safety
    # Focuses on identifying known security vulnerabilities in your project's dependencies by scanning requirements.txt
    # files. It cross-references your dependencies against a curated database of insecure packages to alert you to
    # potential risks.

    # Data Flow
    # Python Taint (PyT)
    #  A static analysis tool designed to detect security vulnerabilities in Python code by tracking the flow of tainted
    #  (untrusted) data to sensitive functions. It helps identify potential injection points and data leaks.

    logger.info(f"Running an iteration series of {bandit_allowance}.")
    while iteration < bandit_allowance:
        logger.info(f"Running iteration {iteration}.")

        # Read the specified file
        try:
            mitigated_file_name = f"{name}_mitigated_iteration_{iteration}{ext}"
            mitigated_file_path = os.path.join(mitigated_folder, mitigated_file_name)
            with open(mitigated_file_path, 'r') as file:
                mitigated_code = file.read()
                logger.info(f"Successfully read file: {mitigated_file_path}")
        except FileNotFoundError:
            logger.error(f"The file '{mitigated_file_path}' was not found.")
            exit(1)
        except IOError as e:
            logger.error(f"An error occurred while reading the file '{mitigated_file_path}': {e}")
            exit(1)

        # Define the system and user prompts
        bandit_system_prompt = (
            "You are an AI programming assistant specializing in secure coding practices."
        )

        # Calculate increments per iteration
        line_count_increment = initial_line_count * 0.5 / bandit_allowance
        word_count_increment = initial_word_count * 0.5 / bandit_allowance

        # Adjust constraints dynamically based on the current iteration
        line_count = int(initial_line_count * 1.1 + line_count_increment * iteration)
        word_count = int(initial_word_count * 1.2 + word_count_increment * iteration)

        # Construct the substitution part of the code prompt
        if suggestion_map:
            substitution_statements = '\n'.join(
                [f"\t* For '{vuln_module}', {guidance}" for vuln_module, guidance in suggestion_map.items()]
            )
            substitution_code_instruction = (
                f"Please provide an adjusted secure version of the code below, addressing the following issues:\n"
                f"{substitution_statements}\n"
            )
        else:
            substitution_code_instruction = (
                "Please provide an adjusted secure version of the code below, addressing the identified issues.\n"
            )

        # Construct the complete prompt
        bandit_issues_section = (f"Bandit has identified the following issues"
                                 f" in the provided Python code:\n{bandit_issues}\n\n") if bandit_issues else ""
        dodgy_issues_section = (f"Dodgy has identified the following issues"
                                f" in the provided Python code:\n{dodgy_issues}\n\n") if dodgy_issues else ""
        semgrep_issues_section = (f"Semgrep has identified the following issues"
                                f" in the provided Python code:\n{semgrep_issues}\n\n") if semgrep_issues else ""

        bandit_code_prompt = (
            f"Ensure that all recommendations adhere to best security practices.\n"
            f"{bandit_issues_section}"
            f"{dodgy_issues_section}"
            f"{semgrep_issues_section}"
            f"{substitution_code_instruction}\n"
            f"Ensure the code does not exceed {word_count} words.\n"
            f"Ensure the code does not exceed {line_count} lines.\n\n"
            f"Only include the code, along with appropriate docstrings and inline comments:\n\n"
            f"```python\n{mitigated_code}\n```"
        )

        messages = [
            {"role": "system", "content": bandit_system_prompt},
            {"role": "user", "content": bandit_code_prompt}
        ]

        # Adjust temperature to control randomness; increase to make output more diverse
        temperature = min(max_temperature, initial_temperature + temperature_increment * iteration)

        # Generate code with adjusted parameters
        """
        Parameters for adjustment
        ----------
        temperature : float, optional
            Controls the randomness of the output, with a range from 0 to 2. Lower values (e.g., 0.2) produce more 
            focused and deterministic responses, while higher values (e.g., 0.8) yield more varied and creative outputs.
             It is generally recommended to adjust either `temperature` or `top_p`, but not both simultaneously. 

        top_p : float, optional
            Also known as nucleus sampling, this parameter ranges from 0 to 1 and determines the diversity of the output
             by considering only the tokens that comprise the top `p` probability mass. For instance, a `top_p` of 0.1 
             means only the tokens within the top 10% probability mass are considered. Adjusting `top_p` can influence 
             the creativity of the response, with lower values leading to more focused outputs.

        top_k : int, optional
            Limits the next token selection to the top `k` tokens with the highest probabilities. Setting `top_k` to 0 
            effectively disables this filtering, allowing the model to consider all possible tokens. Adjusting `top_k` 
            can control the diversity of the output, with lower values leading to more focused and deterministic 
            responses.
        """
        logger.info(f"Using an adjusted temperature of {temperature}.")
        bandit_adjusted_code_response = llm.create_chat_completion(
            messages=messages,  # system and user prompt
            temperature=temperature,
            top_p=1,
            top_k=0,
            stream=True,  # Enable streaming
            stop=["<|endoftext|>"]  # Stop generation at the end-of-text token
        )

        # Process the streamed response
        adjusted_code_block = process_streamed_output(bandit_adjusted_code_response, logger)
        logger.info("Secure code generation completed.")

        # Extract the code block from the secure code suggestion
        code_block = extract_code_block(adjusted_code_block)

        # Extract the libraries from the mitigated code
        extracted_libraries = extract_libraries(code_block)

        # Filter the module.json dictionary
        suggestion_map = {key: value for key, value in module_associations.items() if key in extracted_libraries}

        if code_block:
            name, ext = os.path.splitext(base_name)
            iteration += 1
            mitigated_file_name = f"{name}_mitigated_iteration_{iteration}{ext}"
            mitigated_file_path = os.path.join(mitigated_folder, mitigated_file_name)
            logger.info(f"Applied autoimport to {mitigated_file_path}; imports have been updated.")
            fixed_code = autoimport.fix_code(code_block)
            logger.info(f"Saving code to file {mitigated_file_path}.")
            save_code_to_file(fixed_code, mitigated_file_path, logger)
        else:
            logger.warning("No adjusted code block found in the secure code suggestion. Stopping to diagnose issue.")
            break

        # Perform scans again
        bandit_scan_json_path = analyze_file_with_bandit(mitigated_file_path, logger)
        bandit_issues = get_bandit_issues(bandit_scan_json_path)

        dodgy_scan_json_path = analyze_file_with_dodgy(mitigated_file_path, logger)
        dodgy_issues = get_dodgy_issues(bandit_scan_json_path)

        semgrep_scan_json_path = analyze_file_with_semgrep(mitigated_file_path, extracted_libraries, logger)
        semgrep_issues = get_semgrep_issues(semgrep_scan_json_path)

        if bandit_issues == '' and dodgy_issues == 'No issues found.' and semgrep_issues == 'No issues found.':
            old_mitigated_file_path = os.path.join(mitigated_folder, mitigated_file_name)
            mitigated_file_name = f"{name}_mitigated_iteration_final{ext}"
            new_mitigated_file_path = os.path.join(mitigated_folder, mitigated_file_name)
            os.rename(old_mitigated_file_path, new_mitigated_file_path)
            break

else:
    logger.warning("No code block found in the secure code suggestion.")
