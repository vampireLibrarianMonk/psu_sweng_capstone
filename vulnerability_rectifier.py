import argparse
import os

import autoimport
from llama_cpp import Llama

from utilities_llm import (setup_logger,
                           get_optimal_gpu_layers,
                           get_block_count_keys,
                           validate_bandit_allowance,
                           parse_python_script, run_mitigation_loop,
                           organize_imports_and_globals, write_to_file, process_streamed_output, extract_code_block
                           )

mitigated_folder = "mitigated_files"

os.makedirs(mitigated_folder, exist_ok=True)

# Specify the path to your GGUF model file
model_path = "models/llama-3.2-3b-instruct-q8_0.gguf"

# Set up argument parser
parser = argparse.ArgumentParser(description='Process a Python file for vulnerability assessment.')
parser.add_argument('file_path', help='Path to the Python file to be analyzed.')
parser.add_argument('bandit_allowance', type=validate_bandit_allowance, help='Allowed number of Bandit '
                                                                             'mitigation runs (must be a positive '
                                                                             'integer).')

# Parse command-line arguments
args = parser.parse_args()

input_file_path = args.file_path
bandit_allowance = args.bandit_allowance

# Get and separate file name into its base name and extension
base_name = os.path.basename(input_file_path)
name, ext = os.path.splitext(base_name)

# Initialize the logger
logger = setup_logger(f"vulnerability_rectifier_{name}")

# Read the specified file and use ast to parse it into a map for quicker analysis and mitigation
parsed_script = parse_python_script(input_file_path)

# Get the total layers from the input model
total_layers = get_block_count_keys(model_path, logger)

# Determine the optimal number of GPU layers to offload
n_gpu_layers = get_optimal_gpu_layers(model_path, total_layers)

# Initialize the Llama model with appropriate settings
llm = Llama(
    model_path=model_path,
    seed=42,  # Fixed seed for reproducibility
    n_ctx=4096,  # Set the desired context size here
    use_mmap=True,  # Memory mapping for efficiency
    use_mlock=True,  # Prevent swapping to disk for consistent performance
    n_gpu_layers=n_gpu_layers
)

new_parsed_script = {
    'global_variables': '',
    'methods': {},
    'main_script': ''
}

for section, original_code in parsed_script.items():
    logger.info(f"Processing section: {section}...")

    if section == 'methods' and parsed_script['methods']:
        for method, method_code in parsed_script['methods'].items():
            # Mitigate Code
            new_code_block = run_mitigation_loop(
                bandit_allowance,
                base_name,
                llm,
                logger,
                mitigated_folder,
                method_code,
                method)

            # Update code via input section
            new_parsed_script['methods'][method] = new_code_block

    elif parsed_script[section]:
        new_code_block = run_mitigation_loop(
            bandit_allowance,
            base_name,
            llm,
            logger,
            mitigated_folder,
            original_code,
            section)

        # Update code via input section
        new_parsed_script[section] = new_code_block

    else:
        logger.info(f"No code found in section {section}.")

# Check if there is any code parsed
is_empty = (new_parsed_script['global_variables'] == ''
            and new_parsed_script['main_script'] == ''
            and not new_parsed_script['methods'])

# Check if there is new code to integrate
if not is_empty:
    # Define base name for mitigated files
    mitigated_base_name = f"{name}_mitigated"

    # Define filenames and paths for consolidated and organized code files
    consolidated_file_name = f"{mitigated_base_name}_consolidated{ext}"
    consolidated_file_path = os.path.join(mitigated_folder, consolidated_file_name)

    # Write the consolidated code sections into a single file
    with open(consolidated_file_path, 'w') as file:
        # Write global variables section if it exists
        if new_parsed_script['global_variables']:
            file.write(new_parsed_script['global_variables'] + '\n\n')

        # Write methods in alphabetical order
        for method_name in sorted(new_parsed_script['methods']):
            file.write(new_parsed_script['methods'][method_name] + '\n\n')

        # Write main script section if it exists
        if new_parsed_script['main_script']:
            file.write(new_parsed_script['main_script'] + '\n')

    # Organize imports and global variables in the consolidated code
    organized_code = organize_imports_and_globals(consolidated_file_path)

    # Define filename and path for the organized code file
    organized_file_name = f"{mitigated_base_name}_organized{ext}"
    organized_file_path = os.path.join(mitigated_folder, organized_file_name)

    # Write the organized code to a file
    write_to_file(organized_file_path, organized_code)

    # Read the contents of the organized file
    with open(organized_file_path, 'r') as file:
        code_content = file.read()

    # Remove duplicate import statements using autoimport
    fixed_code = autoimport.fix_code(code_content)

    # Define filename and path for the final output file
    final_file_name = f"{mitigated_base_name}_final{ext}"
    final_file_path = os.path.join(mitigated_folder, final_file_name)

    # Write the fixed code to the final output file
    write_to_file(final_file_path, fixed_code)

    # Specify the path to the GGUF model file
    model_path = "models/llama-3.2-3b-instruct-q8_0.gguf"

    # Retrieve the total number of layers in the model
    total_layers = get_block_count_keys(model_path, logger)

    # Determine the optimal number of layers to offload to the GPU
    n_gpu_layers = get_optimal_gpu_layers(model_path, total_layers)

    # Initialize the Llama model with specified settings
    llm = Llama(
        model_path=model_path,  # Path to the GGUF model file
        seed=42,  # Fixed seed for reproducibility
        n_ctx=4096,  # Context size for the model
        use_mmap=True,  # Enable memory mapping for efficiency
        use_mlock=True,  # Prevent swapping to disk for consistent performance
        n_gpu_layers=n_gpu_layers  # Number of layers to offload to GPU
    )

    # Prepare the system prompt for the AI assistant
    system_prompt = (
        f"You are an AI programming assistant proficient in Python application development. You excel at identifying and"
        f" refactoring code to improve security, readability, and efficiency while adhering to best practices. Your "
        f"guidance must be concise, actionable, and prioritize abstraction when reviewing and refactoring the code."
    )

    # Prepare the user prompt with the code to be reviewed
    user_prompt = (
        f"Please review and refactor the following Python application code. Focus on abstracting redundant logic and "
        f"improving maintainability without changing the intended behavior or adding any new variables. "
        f"Use concise comments to explain critical modifications:```python\n{fixed_code}\n```"
    )

    # Combine system and user prompts into a message list
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    # Generate code with adjusted parameters
    """
    Parameters for adjustment
    ----------
    temperature : float, optional
        Controls the randomness of the output, with a range from 0 to 2. Lower values (e.g., 0.2) produce more 
        focused and deterministic responses, while higher values (e.g., 0.8) yield more varied and creative outputs.
         It is generally recommended to adjust either `temperature` or `top_p`, but not both simultaneously. 

    top_p : float, optional
        Also known as nucleus sampling, this parameter ranges from 0 to 1 and determines the diversity of the output
         by considering only the tokens that comprise the top `p` probability mass. For instance, a `top_p` of 0.1 
         means only the tokens within the top 10% probability mass are considered. Adjusting `top_p` can influence 
         the creativity of the response, with lower values leading to more focused outputs.

    top_k : int, optional
        Limits the next token selection to the top `k` tokens with the highest probabilities. Setting `top_k` to 0 
        effectively disables this filtering, allowing the model to consider all possible tokens. Adjusting `top_k` 
        can control the diversity of the output, with lower values leading to more focused and deterministic 
        responses.
    """
    response = llm.create_chat_completion(
        messages=messages,
        temperature=0.0,  # Set temperature for deterministic output
        top_p=1,  # Use nucleus sampling with top_p probability
        top_k=0,  # Disable top-k sampling
        stream=True,  # Enable streaming of the response
        stop=["<|endoftext|>"]  # Define stopping criteria for generation
    )

    # Process the streamed output to obtain the adjusted code block
    adjusted_code_block = process_streamed_output(response)

    # Extract the code block from the AI's response
    extracted_code_block = extract_code_block(adjusted_code_block)

    # Save the refined code to the final output file
    write_to_file(final_file_path, extracted_code_block)
